{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing dependencies\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from tensorflow.keras.models import Sequential\n",
    "from numpy import array, argmax, random, take\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import RNN, SimpleRNN, LSTM,  Embedding, RepeatVector\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "#For plotting the matplotlib graphs in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data_path=\"D:/Google Drive/Training/Book/0.Chapters/Chapter12 RNN and LSTM/Datasets\"\n",
    "Data_path='Datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data (5351, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "column_names = ['word1', 'word2', 'word3']\n",
    "\n",
    "input_3gram = pd.read_csv(Data_path+ \"/3Gram_love_data.txt\", delimiter='\\t', names=column_names) #Importing csv file with column names\n",
    "print(\"shape of data\", input_3gram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few sample records from data \n",
      "       word1 word2  word3\n",
      "1391   love   him    and\n",
      "2875   love    to    see\n",
      "4941  loved   you   very\n",
      "604    love   the  smell\n",
      "1956   love    of   your\n",
      "4072   love    it   when\n",
      "2560   love   the    way\n",
      "4243   love    it   when\n",
      "4447   love    it   when\n",
      "3495   love    to    see\n"
     ]
    }
   ],
   "source": [
    "print(\"Few sample records from data \\n\", input_3gram.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frequency of word1 values \n",
      " love      4327\n",
      "loved      416\n",
      "hate       400\n",
      "hated       80\n",
      "loves       72\n",
      "lovely      24\n",
      "loving      24\n",
      "hates        8\n",
      "Name: word1, dtype: int64\n",
      "\n",
      "Frequency of word2 values \n",
      " to         1866\n",
      "it         1361\n",
      "the         548\n",
      "with        240\n",
      "you         144\n",
      "him         144\n",
      "of          136\n",
      "her         104\n",
      "for          96\n",
      "and          88\n",
      "what         56\n",
      "is           48\n",
      "each         40\n",
      "in           40\n",
      "ones         32\n",
      "them         32\n",
      "me           32\n",
      "nothing      32\n",
      "every        24\n",
      "as           24\n",
      "more         16\n",
      "going        16\n",
      "my           16\n",
      "that         16\n",
      "being        16\n",
      "affair       16\n",
      "story         8\n",
      "view          8\n",
      "got           8\n",
      "on            8\n",
      "when          8\n",
      "husband       8\n",
      "this          8\n",
      "letter        8\n",
      "about         8\n",
      "all           8\n",
      "your          8\n",
      "at            8\n",
      "man           8\n",
      "most          8\n",
      "thy           8\n",
      "song          8\n",
      "hearing       8\n",
      "makes         8\n",
      "a             8\n",
      "one           8\n",
      "lost          8\n",
      "Name: word2, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFrequency of word1 values \\n\", input_3gram[\"word1\"].value_counts())\n",
    "print(\"\\nFrequency of word2 values \\n\", input_3gram[\"word2\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of unique words overall: 139\n",
      "unique words list: ['a' 'able' 'about' 'admit' 'affair' 'affection' 'all' 'and' 'another'\n",
      " 'answer' 'as' 'at' 'be' 'because' 'being' 'better' 'between' 'bother'\n",
      " 'break' 'care' 'cared' 'come' 'concern' 'country' 'cut' 'disappoint' 'do'\n",
      " 'each' 'every' 'fact' 'feel' 'feeling' 'find' 'first' 'for' 'from' 'get'\n",
      " 'go' 'god' 'going' 'got' 'hate' 'hated' 'hates' 'have' 'he' 'hear'\n",
      " 'hearing' 'her' 'here' 'him' 'his' 'husband' 'i' 'idea' 'if' 'in'\n",
      " 'interrupt' 'is' 'it' 'kind' 'know' 'leave' 'letter' 'life' 'like'\n",
      " 'listen' 'look' 'lost' 'lot' 'love' 'loved' 'lovely' 'loves' 'loving'\n",
      " 'make' 'makes' 'man' 'marriage' 'me' 'minute' 'more' 'most' 'much'\n",
      " 'music' 'my' 'nature' 'neighbor' 'not' 'nothing' 'of' 'on' 'one' 'ones'\n",
      " 'or' 'other' 'over' 'play' 'respect' 'say' 'see' 'sit' 'smell' 'so'\n",
      " 'someone' 'song' 'sound' 'story' 'stronger' 'support' 'take' 'talk'\n",
      " 'tell' 'than' 'that' 'the' 'them' 'they' 'think' 'this' 'thought' 'thy'\n",
      " 'to' 'too' 'united' 'use' 'very' 'view' 'watch' 'way' 'we' 'what' 'when'\n",
      " 'wife' 'will' 'with' 'work' 'you' 'your']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Finding our words to create dictionary\n",
    "Here we find unique values in each column and save each of those values .\n",
    "Later which we will take the unique value for the entire appened columns\n",
    "This will be our vocabulary list,which are the unique words in our data file\n",
    "\"\"\"\n",
    "unique_words = []\n",
    "for i in list(input_3gram.columns.values):\n",
    "    for j in pd.unique(input_3gram[i]):\n",
    "        unique_words.append(j)\n",
    "unique_words = np.unique(unique_words)\n",
    "\n",
    "\n",
    "print('Count of unique words overall:', len(unique_words))\n",
    "print('unique words list:', unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_indices dictionary \n",
      " {'a': 0, 'able': 1, 'about': 2, 'admit': 3, 'affair': 4, 'affection': 5, 'all': 6, 'and': 7, 'another': 8, 'answer': 9, 'as': 10, 'at': 11, 'be': 12, 'because': 13, 'being': 14, 'better': 15, 'between': 16, 'bother': 17, 'break': 18, 'care': 19, 'cared': 20, 'come': 21, 'concern': 22, 'country': 23, 'cut': 24, 'disappoint': 25, 'do': 26, 'each': 27, 'every': 28, 'fact': 29, 'feel': 30, 'feeling': 31, 'find': 32, 'first': 33, 'for': 34, 'from': 35, 'get': 36, 'go': 37, 'god': 38, 'going': 39, 'got': 40, 'hate': 41, 'hated': 42, 'hates': 43, 'have': 44, 'he': 45, 'hear': 46, 'hearing': 47, 'her': 48, 'here': 49, 'him': 50, 'his': 51, 'husband': 52, 'i': 53, 'idea': 54, 'if': 55, 'in': 56, 'interrupt': 57, 'is': 58, 'it': 59, 'kind': 60, 'know': 61, 'leave': 62, 'letter': 63, 'life': 64, 'like': 65, 'listen': 66, 'look': 67, 'lost': 68, 'lot': 69, 'love': 70, 'loved': 71, 'lovely': 72, 'loves': 73, 'loving': 74, 'make': 75, 'makes': 76, 'man': 77, 'marriage': 78, 'me': 79, 'minute': 80, 'more': 81, 'most': 82, 'much': 83, 'music': 84, 'my': 85, 'nature': 86, 'neighbor': 87, 'not': 88, 'nothing': 89, 'of': 90, 'on': 91, 'one': 92, 'ones': 93, 'or': 94, 'other': 95, 'over': 96, 'play': 97, 'respect': 98, 'say': 99, 'see': 100, 'sit': 101, 'smell': 102, 'so': 103, 'someone': 104, 'song': 105, 'sound': 106, 'story': 107, 'stronger': 108, 'support': 109, 'take': 110, 'talk': 111, 'tell': 112, 'than': 113, 'that': 114, 'the': 115, 'them': 116, 'they': 117, 'think': 118, 'this': 119, 'thought': 120, 'thy': 121, 'to': 122, 'too': 123, 'united': 124, 'use': 125, 'very': 126, 'view': 127, 'watch': 128, 'way': 129, 'we': 130, 'what': 131, 'when': 132, 'wife': 133, 'will': 134, 'with': 135, 'work': 136, 'you': 137, 'your': 138}\n",
      "word_indices.keys \n",
      " dict_keys(['a', 'able', 'about', 'admit', 'affair', 'affection', 'all', 'and', 'another', 'answer', 'as', 'at', 'be', 'because', 'being', 'better', 'between', 'bother', 'break', 'care', 'cared', 'come', 'concern', 'country', 'cut', 'disappoint', 'do', 'each', 'every', 'fact', 'feel', 'feeling', 'find', 'first', 'for', 'from', 'get', 'go', 'god', 'going', 'got', 'hate', 'hated', 'hates', 'have', 'he', 'hear', 'hearing', 'her', 'here', 'him', 'his', 'husband', 'i', 'idea', 'if', 'in', 'interrupt', 'is', 'it', 'kind', 'know', 'leave', 'letter', 'life', 'like', 'listen', 'look', 'lost', 'lot', 'love', 'loved', 'lovely', 'loves', 'loving', 'make', 'makes', 'man', 'marriage', 'me', 'minute', 'more', 'most', 'much', 'music', 'my', 'nature', 'neighbor', 'not', 'nothing', 'of', 'on', 'one', 'ones', 'or', 'other', 'over', 'play', 'respect', 'say', 'see', 'sit', 'smell', 'so', 'someone', 'song', 'sound', 'story', 'stronger', 'support', 'take', 'talk', 'tell', 'than', 'that', 'the', 'them', 'they', 'think', 'this', 'thought', 'thy', 'to', 'too', 'united', 'use', 'very', 'view', 'watch', 'way', 'we', 'what', 'when', 'wife', 'will', 'with', 'work', 'you', 'your'])\n",
      "word_indices.values \n",
      " dict_values([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138])\n",
      "\n",
      " ########################################\n",
      "\n",
      "indices_words dictionary \n",
      " {0: 'a', 1: 'able', 2: 'about', 3: 'admit', 4: 'affair', 5: 'affection', 6: 'all', 7: 'and', 8: 'another', 9: 'answer', 10: 'as', 11: 'at', 12: 'be', 13: 'because', 14: 'being', 15: 'better', 16: 'between', 17: 'bother', 18: 'break', 19: 'care', 20: 'cared', 21: 'come', 22: 'concern', 23: 'country', 24: 'cut', 25: 'disappoint', 26: 'do', 27: 'each', 28: 'every', 29: 'fact', 30: 'feel', 31: 'feeling', 32: 'find', 33: 'first', 34: 'for', 35: 'from', 36: 'get', 37: 'go', 38: 'god', 39: 'going', 40: 'got', 41: 'hate', 42: 'hated', 43: 'hates', 44: 'have', 45: 'he', 46: 'hear', 47: 'hearing', 48: 'her', 49: 'here', 50: 'him', 51: 'his', 52: 'husband', 53: 'i', 54: 'idea', 55: 'if', 56: 'in', 57: 'interrupt', 58: 'is', 59: 'it', 60: 'kind', 61: 'know', 62: 'leave', 63: 'letter', 64: 'life', 65: 'like', 66: 'listen', 67: 'look', 68: 'lost', 69: 'lot', 70: 'love', 71: 'loved', 72: 'lovely', 73: 'loves', 74: 'loving', 75: 'make', 76: 'makes', 77: 'man', 78: 'marriage', 79: 'me', 80: 'minute', 81: 'more', 82: 'most', 83: 'much', 84: 'music', 85: 'my', 86: 'nature', 87: 'neighbor', 88: 'not', 89: 'nothing', 90: 'of', 91: 'on', 92: 'one', 93: 'ones', 94: 'or', 95: 'other', 96: 'over', 97: 'play', 98: 'respect', 99: 'say', 100: 'see', 101: 'sit', 102: 'smell', 103: 'so', 104: 'someone', 105: 'song', 106: 'sound', 107: 'story', 108: 'stronger', 109: 'support', 110: 'take', 111: 'talk', 112: 'tell', 113: 'than', 114: 'that', 115: 'the', 116: 'them', 117: 'they', 118: 'think', 119: 'this', 120: 'thought', 121: 'thy', 122: 'to', 123: 'too', 124: 'united', 125: 'use', 126: 'very', 127: 'view', 128: 'watch', 129: 'way', 130: 'we', 131: 'what', 132: 'when', 133: 'wife', 134: 'will', 135: 'with', 136: 'work', 137: 'you', 138: 'your'}\n",
      "indices_words keys \n",
      " dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138])\n",
      "indices_words values \n",
      " dict_values(['a', 'able', 'about', 'admit', 'affair', 'affection', 'all', 'and', 'another', 'answer', 'as', 'at', 'be', 'because', 'being', 'better', 'between', 'bother', 'break', 'care', 'cared', 'come', 'concern', 'country', 'cut', 'disappoint', 'do', 'each', 'every', 'fact', 'feel', 'feeling', 'find', 'first', 'for', 'from', 'get', 'go', 'god', 'going', 'got', 'hate', 'hated', 'hates', 'have', 'he', 'hear', 'hearing', 'her', 'here', 'him', 'his', 'husband', 'i', 'idea', 'if', 'in', 'interrupt', 'is', 'it', 'kind', 'know', 'leave', 'letter', 'life', 'like', 'listen', 'look', 'lost', 'lot', 'love', 'loved', 'lovely', 'loves', 'loving', 'make', 'makes', 'man', 'marriage', 'me', 'minute', 'more', 'most', 'much', 'music', 'my', 'nature', 'neighbor', 'not', 'nothing', 'of', 'on', 'one', 'ones', 'or', 'other', 'over', 'play', 'respect', 'say', 'see', 'sit', 'smell', 'so', 'someone', 'song', 'sound', 'story', 'stronger', 'support', 'take', 'talk', 'tell', 'than', 'that', 'the', 'them', 'they', 'think', 'this', 'thought', 'thy', 'to', 'too', 'united', 'use', 'very', 'view', 'watch', 'way', 'we', 'what', 'when', 'wife', 'will', 'with', 'work', 'you', 'your'])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "creating our word:indice pair dictionary and inverse\n",
    "Here will be creating two dictonary values\n",
    "word_indices : This contains each words mapped to an unique digit \n",
    "indices_words : This contains each digits mapped to a word in the same sequence as word_indices \n",
    "\"\"\"\n",
    "word_indices = dict((w, i) for i, w in enumerate(unique_words))\n",
    "indices_words = dict((i, w) for i, w in enumerate(unique_words))\n",
    "\n",
    "print(\"word_indices dictionary \\n\",word_indices)\n",
    "print(\"word_indices.keys \\n\", word_indices.keys())\n",
    "print(\"word_indices.values \\n\", word_indices.values())\n",
    "print(\"\\n ########################################\\n\")\n",
    "print(\"indices_words dictionary \\n\", indices_words)\n",
    "print(\"indices_words keys \\n\",indices_words.keys())\n",
    "print(\"indices_words values \\n\",indices_words.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word1_onehot shape is  (5351, 139)\n"
     ]
    }
   ],
   "source": [
    "### Onehot encoding of word1\n",
    "word1 = input_3gram['word1'].map(word_indices)\n",
    "word1_onehot = keras.utils.to_categorical(np.array(word1), num_classes=len(word_indices))\n",
    "print(\"word1_onehot shape is \",word1_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word in row 0 is -->hate\n",
      "The one hot encoded version of the word in row 0 is \n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "The word in row 500 is --> love\n",
      "The one hot encoded version of the word in row 500 is \n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Lets take example of two different words\n",
    "print(\"The word in row 0 is -->\"+input_3gram['word1'][0])\n",
    "print(\"The one hot encoded version of the word in row 0 is \\n\",word1_onehot[0])\n",
    "\n",
    "print(\"\\nThe word in row 500 is --> \"+input_3gram['word1'][500])\n",
    "print(\"The one hot encoded version of the word in row 500 is \\n\",word1_onehot[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2_onehot shape is  (5351, 139)\n",
      "word3_onehot shape is  (5351, 139)\n"
     ]
    }
   ],
   "source": [
    "##one hot encoding for word2 and word3 \n",
    "word2 = input_3gram['word2'].map(word_indices)\n",
    "word2_onehot = keras.utils.to_categorical(np.array(word2), num_classes=len(word_indices))\n",
    "print(\"word2_onehot shape is \",word2_onehot.shape)\n",
    "\n",
    "word3 = input_3gram['word3'].map(word_indices)\n",
    "word3_onehot = keras.utils.to_categorical(np.array(word3), num_classes=len(word_indices))\n",
    "print(\"word3_onehot shape is \",word3_onehot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                1400      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 139)               1529      \n",
      "=================================================================\n",
      "Total params: 2,929\n",
      "Trainable params: 2,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ANN_model1 = Sequential()\n",
    "ANN_model1.add(Dense(10, input_dim=word1_onehot.shape[1], activation='sigmoid'))\n",
    "ANN_model1.add(Dense(word2_onehot.shape[1] ,activation='softmax'))\n",
    "ANN_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5351 samples\n",
      "Epoch 1/20\n",
      "5351/5351 [==============================] - 1s 127us/sample - loss: 0.0400 - accuracy: 0.9928\n",
      "Epoch 2/20\n",
      "5351/5351 [==============================] - 0s 27us/sample - loss: 0.0331 - accuracy: 0.9928\n",
      "Epoch 3/20\n",
      "5351/5351 [==============================] - 0s 23us/sample - loss: 0.0272 - accuracy: 0.9928\n",
      "Epoch 4/20\n",
      "5351/5351 [==============================] - 0s 24us/sample - loss: 0.0240 - accuracy: 0.9928\n",
      "Epoch 5/20\n",
      "5351/5351 [==============================] - 0s 25us/sample - loss: 0.0231 - accuracy: 0.9928\n",
      "Epoch 6/20\n",
      "5351/5351 [==============================] - 0s 27us/sample - loss: 0.0228 - accuracy: 0.9928\n",
      "Epoch 7/20\n",
      "5351/5351 [==============================] - 0s 23us/sample - loss: 0.0227 - accuracy: 0.9928\n",
      "Epoch 8/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0226 - accuracy: 0.9928\n",
      "Epoch 9/20\n",
      "5351/5351 [==============================] - 0s 28us/sample - loss: 0.0225 - accuracy: 0.9928\n",
      "Epoch 10/20\n",
      "5351/5351 [==============================] - 0s 25us/sample - loss: 0.0225 - accuracy: 0.9928\n",
      "Epoch 11/20\n",
      "5351/5351 [==============================] - 0s 23us/sample - loss: 0.0224 - accuracy: 0.9928\n",
      "Epoch 12/20\n",
      "5351/5351 [==============================] - 0s 23us/sample - loss: 0.0224 - accuracy: 0.9928\n",
      "Epoch 13/20\n",
      "5351/5351 [==============================] - 0s 23us/sample - loss: 0.0224 - accuracy: 0.9928\n",
      "Epoch 14/20\n",
      "5351/5351 [==============================] - 0s 23us/sample - loss: 0.0223 - accuracy: 0.9928\n",
      "Epoch 15/20\n",
      "5351/5351 [==============================] - 0s 24us/sample - loss: 0.0223 - accuracy: 0.9928\n",
      "Epoch 16/20\n",
      "5351/5351 [==============================] - 0s 23us/sample - loss: 0.0223 - accuracy: 0.9928\n",
      "Epoch 17/20\n",
      "5351/5351 [==============================] - 0s 23us/sample - loss: 0.0223 - accuracy: 0.9928\n",
      "Epoch 18/20\n",
      "5351/5351 [==============================] - 0s 23us/sample - loss: 0.0223 - accuracy: 0.9928\n",
      "Epoch 19/20\n",
      "5351/5351 [==============================] - 0s 24us/sample - loss: 0.0222 - accuracy: 0.9928\n",
      "Epoch 20/20\n",
      "5351/5351 [==============================] - 0s 26us/sample - loss: 0.0222 - accuracy: 0.9928\n"
     ]
    }
   ],
   "source": [
    "ANN_model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train model\n",
    "history = ANN_model1.fit(word1_onehot, word2_onehot, epochs=20, batch_size=50,  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will see what the 1st hidden layer output representation of the data  \n",
    "# to predict the hidden layer activations, \n",
    "# let's rewrite first layer of our model and give it the weights from fully trained previous model\n",
    "model1_hidden = Sequential()\n",
    "model1_hidden.add(Dense(10, input_dim=word1_onehot.shape[1], weights=ANN_model1.layers[0].get_weights()))\n",
    "model1_hidden.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hidden layer output for every record - Shape of it \n",
      " (5351, 10)\n",
      "Few five records from hidden layer \n",
      " [[0.85935473 0.85981625 0.8405648  0.73825604 0.746676   0.8174511\n",
      "  0.8211262  0.85222316 0.7907549  0.8912274 ]\n",
      " [0.85935473 0.85981625 0.8405648  0.73825604 0.746676   0.8174511\n",
      "  0.8211262  0.85222316 0.7907549  0.8912274 ]\n",
      " [0.85935473 0.85981625 0.8405648  0.73825604 0.746676   0.8174511\n",
      "  0.8211262  0.85222316 0.7907549  0.8912274 ]\n",
      " [0.85935473 0.85981625 0.8405648  0.73825604 0.746676   0.8174511\n",
      "  0.8211262  0.85222316 0.7907549  0.8912274 ]\n",
      " [0.85935473 0.85981625 0.8405648  0.73825604 0.746676   0.8174511\n",
      "  0.8211262  0.85222316 0.7907549  0.8912274 ]]\n"
     ]
    }
   ],
   "source": [
    "# Getting the hidden layer activations\n",
    "model1_hidden_output = model1_hidden.predict(word1_onehot)\n",
    "#peak into our hidden layer activations\n",
    "print(\"The hidden layer output for every record - Shape of it \\n\", model1_hidden_output.shape)\n",
    "print(\"Few five records from hidden layer \\n\",model1_hidden_output[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2_hidden_append Shape (5351, 149)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We append the input words of the words2 column in the output of the h1 layer,this gives us the combined input representation\n",
    "\"\"\"\n",
    "word2_hidden_append = np.append(model1_hidden_output,word2_onehot, axis=1)\n",
    "print(\"word2_hidden_append Shape\", word2_hidden_append.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10)                1500      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 139)               1529      \n",
      "=================================================================\n",
      "Total params: 3,029\n",
      "Trainable params: 3,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ANN_model2 = Sequential()\n",
    "ANN_model2.add(Dense(10, input_dim=word2_hidden_append.shape[1], activation='sigmoid'))\n",
    "ANN_model2.add(Dense(word3_onehot.shape[1], activation='softmax'))\n",
    "ANN_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5351 samples\n",
      "Epoch 1/20\n",
      "5351/5351 [==============================] - 1s 94us/sample - loss: 0.0402 - accuracy: 0.9928\n",
      "Epoch 2/20\n",
      "5351/5351 [==============================] - 0s 25us/sample - loss: 0.0344 - accuracy: 0.9928\n",
      "Epoch 3/20\n",
      "5351/5351 [==============================] - 0s 25us/sample - loss: 0.0309 - accuracy: 0.9928\n",
      "Epoch 4/20\n",
      "5351/5351 [==============================] - 0s 25us/sample - loss: 0.0303 - accuracy: 0.9928\n",
      "Epoch 5/20\n",
      "5351/5351 [==============================] - 0s 24us/sample - loss: 0.0301 - accuracy: 0.9928\n",
      "Epoch 6/20\n",
      "5351/5351 [==============================] - 0s 24us/sample - loss: 0.0299 - accuracy: 0.9928\n",
      "Epoch 7/20\n",
      "5351/5351 [==============================] - 0s 23us/sample - loss: 0.0297 - accuracy: 0.9928\n",
      "Epoch 8/20\n",
      "5351/5351 [==============================] - 0s 24us/sample - loss: 0.0293 - accuracy: 0.9928\n",
      "Epoch 9/20\n",
      "5351/5351 [==============================] - 0s 24us/sample - loss: 0.0288 - accuracy: 0.9928\n",
      "Epoch 10/20\n",
      "5351/5351 [==============================] - 0s 24us/sample - loss: 0.0283 - accuracy: 0.9928\n",
      "Epoch 11/20\n",
      "5351/5351 [==============================] - 0s 29us/sample - loss: 0.0276 - accuracy: 0.9928\n",
      "Epoch 12/20\n",
      "5351/5351 [==============================] - 0s 23us/sample - loss: 0.0270 - accuracy: 0.9944\n",
      "Epoch 13/20\n",
      "5351/5351 [==============================] - 0s 23us/sample - loss: 0.0263 - accuracy: 0.9945\n",
      "Epoch 14/20\n",
      "5351/5351 [==============================] - 0s 25us/sample - loss: 0.0257 - accuracy: 0.9945\n",
      "Epoch 15/20\n",
      "5351/5351 [==============================] - 0s 23us/sample - loss: 0.0252 - accuracy: 0.9945\n",
      "Epoch 16/20\n",
      "5351/5351 [==============================] - 0s 24us/sample - loss: 0.0247 - accuracy: 0.9945\n",
      "Epoch 17/20\n",
      "5351/5351 [==============================] - 0s 24us/sample - loss: 0.0242 - accuracy: 0.9945\n",
      "Epoch 18/20\n",
      "5351/5351 [==============================] - 0s 24us/sample - loss: 0.0238 - accuracy: 0.9946\n",
      "Epoch 19/20\n",
      "5351/5351 [==============================] - 0s 24us/sample - loss: 0.0234 - accuracy: 0.9949\n",
      "Epoch 20/20\n",
      "5351/5351 [==============================] - 0s 24us/sample - loss: 0.0231 - accuracy: 0.9949\n"
     ]
    }
   ],
   "source": [
    "ANN_model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train model\n",
    "history = ANN_model2.fit(word2_hidden_append, word3_onehot, epochs=20, batch_size=50,  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A predict function that takes input word1 and word2; and predict word3 \n",
    "#1. take the input word , and represent them using digits from the word_indices dictonary values\n",
    "#2. getting the intermediate hidden nodes for word1\n",
    "#3. appending hidden activations with word2 as final test set\n",
    "#4. prediction on this test set\n",
    "def two_step_pred(words_in):\n",
    "\n",
    "    index_input=word_indices[words_in[0]]\n",
    "    indices_in = keras.utils.to_categorical(index_input, num_classes=len(word_indices))\n",
    "    indices_in=indices_in.reshape(1,len(word_indices))\n",
    "    h1_test = model1_hidden.predict(indices_in) # getting our intermediate hidden activations from model1h\n",
    "    \n",
    "    \n",
    "    index_input2=word_indices[words_in[1]]\n",
    "    indices_in2 = keras.utils.to_categorical(index_input2, num_classes=len(word_indices))\n",
    "    indices_in2= indices_in2.reshape(1,len(word_indices))\n",
    "    X2_test = np.append(h1_test, indices_in2, axis=1) #preparing final test data by appending hidden with word2\n",
    "    \n",
    "    yhat = ANN_model2.predict_classes(X2_test) #predicting final output from model2\n",
    "    \n",
    "    print(\"Input words --> \", words_in)\n",
    "    print(\"Predicted word --> \", indices_words[yhat[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input words -->  ['love', 'it']\n",
      "Predicted word -->  when\n",
      "Input words -->  ['love', 'to']\n",
      "Predicted word -->  see\n",
      "Input words -->  ['love', 'the']\n",
      "Predicted word -->  way\n"
     ]
    }
   ],
   "source": [
    "two_step_pred(['love', 'it'])\n",
    "two_step_pred(['love', 'to'])\n",
    "two_step_pred(['love', 'the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, 4)                 24        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 12        \n",
      "=================================================================\n",
      "Total params: 36\n",
      "Trainable params: 36\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(4, use_bias=False, input_shape=(2,2)))\n",
    "model.add(Dense(3, use_bias=False, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 43\n",
      "Trainable params: 43\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(4, input_shape=(2,2)))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 43\n",
      "Trainable params: 43\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(4, input_shape=(4,2)))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word prediction using RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word1_word2_onehot shape (5351, 2, 139)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shuang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "word1_word2 = input_3gram[['word1','word2']]\n",
    "for i in list(word1_word2.columns.values):\n",
    "    word1_word2[i] = word1_word2[i].map(word_indices)\n",
    "\n",
    "word1_word2=np.array(word1_word2)\n",
    "#The same data is reshaped with similar structure but appended with 1 value to make it 3d array\n",
    "word1_word2=np.reshape(word1_word2,(word1_word2.shape[0],2,1))\n",
    "word1_word2_onehot = keras.utils.to_categorical(np.array(word1_word2), num_classes=len(word_indices))\n",
    "print(\"word1_word2_onehot shape\", word1_word2_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time steps 2\n",
      "Input nodes 139\n",
      "output nodes 139\n"
     ]
    }
   ],
   "source": [
    "print(\"time steps\" , word1_word2_onehot.shape[1])\n",
    "print(\"Input nodes\" , word1_word2_onehot.shape[2])\n",
    "print(\"output nodes\" , word3_onehot.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_3 (SimpleRNN)     (None, 30)                5100      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 139)               4309      \n",
      "=================================================================\n",
      "Total params: 9,409\n",
      "Trainable params: 9,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_rnn = Sequential()\n",
    "#model.add(SimpleRNN('number of hidden nodes in each rnn cell', input_shape=(timesteps, input_data_dim)))\n",
    "model_rnn.add(SimpleRNN(30, input_shape=(word1_word2_onehot.shape[1],word1_word2_onehot.shape[2]))) \n",
    "model_rnn.add(Dense(word3_onehot.shape[1], activation='softmax'))\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5351 samples\n",
      "Epoch 1/20\n",
      "5351/5351 [==============================] - 1s 188us/sample - loss: 3.7871 - accuracy: 0.3730\n",
      "Epoch 2/20\n",
      "5351/5351 [==============================] - 0s 48us/sample - loss: 2.7493 - accuracy: 0.4964\n",
      "Epoch 3/20\n",
      "5351/5351 [==============================] - 0s 52us/sample - loss: 2.3492 - accuracy: 0.5367\n",
      "Epoch 4/20\n",
      "5351/5351 [==============================] - 0s 50us/sample - loss: 2.0732 - accuracy: 0.5657\n",
      "Epoch 5/20\n",
      "5351/5351 [==============================] - 0s 48us/sample - loss: 1.8817 - accuracy: 0.5894\n",
      "Epoch 6/20\n",
      "5351/5351 [==============================] - 0s 48us/sample - loss: 1.7405 - accuracy: 0.6034\n",
      "Epoch 7/20\n",
      "5351/5351 [==============================] - 0s 47us/sample - loss: 1.6309 - accuracy: 0.6242\n",
      "Epoch 8/20\n",
      "5351/5351 [==============================] - 0s 47us/sample - loss: 1.5425 - accuracy: 0.6404\n",
      "Epoch 9/20\n",
      "5351/5351 [==============================] - 0s 52us/sample - loss: 1.4715 - accuracy: 0.6496\n",
      "Epoch 10/20\n",
      "5351/5351 [==============================] - 0s 52us/sample - loss: 1.4133 - accuracy: 0.6574\n",
      "Epoch 11/20\n",
      "5351/5351 [==============================] - 0s 48us/sample - loss: 1.3658 - accuracy: 0.6617\n",
      "Epoch 12/20\n",
      "5351/5351 [==============================] - 0s 49us/sample - loss: 1.3279 - accuracy: 0.6616\n",
      "Epoch 13/20\n",
      "5351/5351 [==============================] - 0s 48us/sample - loss: 1.2977 - accuracy: 0.6619\n",
      "Epoch 14/20\n",
      "5351/5351 [==============================] - 0s 55us/sample - loss: 1.2732 - accuracy: 0.6647\n",
      "Epoch 15/20\n",
      "5351/5351 [==============================] - 0s 57us/sample - loss: 1.2535 - accuracy: 0.6638\n",
      "Epoch 16/20\n",
      "5351/5351 [==============================] - 0s 54us/sample - loss: 1.2367 - accuracy: 0.6638\n",
      "Epoch 17/20\n",
      "5351/5351 [==============================] - 0s 49us/sample - loss: 1.2236 - accuracy: 0.6634\n",
      "Epoch 18/20\n",
      "5351/5351 [==============================] - 0s 48us/sample - loss: 1.2127 - accuracy: 0.6640\n",
      "Epoch 19/20\n",
      "5351/5351 [==============================] - 0s 48us/sample - loss: 1.2044 - accuracy: 0.6640\n",
      "Epoch 20/20\n",
      "5351/5351 [==============================] - 0s 46us/sample - loss: 1.1966 - accuracy: 0.6636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8177cb1a90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile network\n",
    "model_rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model_rnn.fit(word1_word2_onehot, word3_onehot, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_word_pred(in_text):\n",
    "    print(\"Input is - \" , in_text)\n",
    "    encoded = [word_indices[i] for i in in_text]\n",
    "    encoded = np.array(encoded).reshape(1,2,1)\n",
    "    encoded =keras.utils.to_categorical(np.array(encoded), num_classes=len(word_indices))\n",
    "    ypred = model_rnn.predict_classes(encoded, verbose=0)[0]\n",
    "    print(\"Output is --> \" ,indices_words[ypred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input is -  ['love', 'it']\n",
      "Output is -->  when\n",
      "Input is -  ['love', 'to']\n",
      "Output is -->  see\n",
      "Input is -  ['love', 'the']\n",
      "Output is -->  way\n"
     ]
    }
   ],
   "source": [
    "rnn_word_pred(['love', 'it'])\n",
    "rnn_word_pred(['love', 'to'])\n",
    "rnn_word_pred(['love', 'the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN for Long Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,addresses,of\n",
      "and,adherence,to\n",
      "and,advocates,for\n",
      "and,aerospace,engineering\n",
      "and,americans,do\n",
      "and,analyzing,the\n",
      "and,announced,he\n",
      "and,announced,he\n",
      "and,announced,plans\n",
      "and,announced,that\n",
      "and,announced,that\n",
      "and,annou\n"
     ]
    }
   ],
   "source": [
    "longseq_3gram = open(Data_path+'/Long_sequence_3gram.csv').read().lower()\n",
    "print(longseq_3gram[495:801])\n",
    "print(longseq_3gram[30615:31000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and addresses \n"
     ]
    }
   ],
   "source": [
    "#Replace comma with space\n",
    "longseq_3gram1= longseq_3gram.replace(',',' ').replace('\\r','')\n",
    "print(longseq_3gram1[495:750])\n",
    "print(longseq_3gram1[30615:30800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Characters in the text \n",
      "  ['\\n', ' ', \"'\", '(', '-', '.', '/', '0', '1', '3', '7', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      " Character after removing newline symbol '/n' [' ', \"'\", '(', '-', '.', '/', '0', '1', '3', '7', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      " overall chars count 37\n"
     ]
    }
   ],
   "source": [
    "#Unique characters in our dataset we then sort it\n",
    "chars = sorted(list(set(longseq_3gram1)))\n",
    "print(\"Unique Characters in the text \\n \",chars)\n",
    "#\\n is character string for new line, we dont need that in our dictionary of chars\n",
    "chars.remove('\\n')\n",
    "print(\"\\n Character after removing newline symbol \\'/n\\'\",chars)\n",
    "print(\"\\n overall chars count\", len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "characters to indices dictionary\n",
      " {' ': 0, \"'\": 1, '(': 2, '-': 3, '.': 4, '/': 5, '0': 6, '1': 7, '3': 8, '7': 9, '9': 10, 'a': 11, 'b': 12, 'c': 13, 'd': 14, 'e': 15, 'f': 16, 'g': 17, 'h': 18, 'i': 19, 'j': 20, 'k': 21, 'l': 22, 'm': 23, 'n': 24, 'o': 25, 'p': 26, 'q': 27, 'r': 28, 's': 29, 't': 30, 'u': 31, 'v': 32, 'w': 33, 'x': 34, 'y': 35, 'z': 36}\n",
      "indices to char dictionary\n",
      " {0: ' ', 1: \"'\", 2: '(', 3: '-', 4: '.', 5: '/', 6: '0', 7: '1', 8: '3', 9: '7', 10: '9', 11: 'a', 12: 'b', 13: 'c', 14: 'd', 15: 'e', 16: 'f', 17: 'g', 18: 'h', 19: 'i', 20: 'j', 21: 'k', 22: 'l', 23: 'm', 24: 'n', 25: 'o', 26: 'p', 27: 'q', 28: 'r', 29: 's', 30: 't', 31: 'u', 32: 'v', 33: 'w', 34: 'x', 35: 'y', 36: 'z'}\n",
      "unique chars:  {37}\n"
     ]
    }
   ],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "print(\"characters to indices dictionary\\n\", char_indices)\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print(\"indices to char dictionary\\n\", indices_char)\n",
    "print('unique chars: ', {len(chars)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char to index on full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a bewildering array  [11, 0, 12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 11, 28, 28, 11, 35, 0]\n",
      "a celebration of  [11, 0, 12, 15, 24, 15, 16, 19, 13, 19, 11, 28, 35, 0, 25, 16, 0]\n",
      "a co-director of  [11, 0, 12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 32, 11, 28, 19, 15, 30, 35, 0]\n",
      "a declaration of  [11, 0, 12, 19, 30, 30, 15, 28, 29, 33, 15, 15, 30, 0, 23, 25, 23, 15, 24, 30, 0]\n",
      "a significant risk  [11, 0, 29, 19, 17, 24, 19, 16, 19, 13, 11, 24, 30, 0, 28, 19, 29, 21, 0]\n",
      "been designed as  [12, 15, 15, 24, 0, 14, 15, 29, 19, 17, 24, 15, 14, 0, 11, 29, 0]\n",
      "from anywhere on  [16, 28, 25, 23, 0, 11, 24, 35, 33, 18, 15, 28, 15, 0, 25, 24, 0]\n",
      "Number of sentences  30307\n"
     ]
    }
   ],
   "source": [
    "data = longseq_3gram1.splitlines()\n",
    "##Adding a space at the end\n",
    "data = [i+' ' for i in data]\n",
    "\n",
    "##mapping our data into numbers\n",
    "sentences = [[char_indices[j] for j in i] for i in data ]\n",
    "print(data[0], sentences[0])\n",
    "print(data[10], sentences[1])\n",
    "print(data[20], sentences[2])\n",
    "print(data[100], sentences[3])\n",
    "print(data[400], sentences[400])\n",
    "print(data[4000], sentences[4000])\n",
    "print(data[9000], sentences[9000])\n",
    "##Number of sentences\n",
    "print(\"Number of sentences \", len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the sentences to RNN friendly data which can be used to generate new sequences\n",
    "\n",
    "**Take one sentence, iterate through it till the length of sentence is reached:**\n",
    "\n",
    "* **Step 1** 0:0+14: X; and 14th position: y >> observation 1\n",
    "* **Step 2** 1:14: X; and 15th position: y >> observation 2\n",
    "* **Step 3** â€¦we do this till the length of sentence is reached\n",
    "\n",
    "Take next sentence and repeat the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142142, 142142)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since all the sentences may not be of same length,it is neccessary to make them consistent when passing to keras\n",
    "#We select a sequence length\n",
    "Seq_ln = 14\n",
    "X = []\n",
    "y = []\n",
    "for i in sentences:\n",
    "    for j in range(len(i)-Seq_ln):\n",
    "        X.append(i[j:j+Seq_ln])\n",
    "        y.append(i[j+Seq_ln])\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0:2]= ['a bewildering array ', 'a beneficiary of ']\n",
      "sentences[0:2]= [[11, 0, 12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 11, 28, 28, 11, 35, 0], [11, 0, 12, 15, 24, 15, 16, 19, 13, 19, 11, 28, 35, 0, 25, 16, 0]]\n",
      "X[ 0 ]= [11, 0, 12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0] y[ 0 ]= 11\n",
      "X[ 1 ]= [0, 12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 11] y[ 1 ]= 28\n",
      "X[ 2 ]= [12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 11, 28] y[ 2 ]= 28\n",
      "X[ 3 ]= [15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 11, 28, 28] y[ 3 ]= 11\n",
      "X[ 4 ]= [33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 11, 28, 28, 11] y[ 4 ]= 35\n",
      "X[ 5 ]= [19, 22, 14, 15, 28, 19, 24, 17, 0, 11, 28, 28, 11, 35] y[ 5 ]= 0\n",
      "X[ 6 ]= [11, 0, 12, 15, 24, 15, 16, 19, 13, 19, 11, 28, 35, 0] y[ 6 ]= 25\n",
      "X[ 7 ]= [0, 12, 15, 24, 15, 16, 19, 13, 19, 11, 28, 35, 0, 25] y[ 7 ]= 16\n",
      "X[ 8 ]= [12, 15, 24, 15, 16, 19, 13, 19, 11, 28, 35, 0, 25, 16] y[ 8 ]= 0\n",
      "X[ 9 ]= [11, 0, 12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0] y[ 9 ]= 32\n",
      "X[ 10 ]= [0, 12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 32] y[ 10 ]= 11\n",
      "X[ 11 ]= [12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 32, 11] y[ 11 ]= 28\n",
      "X[ 12 ]= [15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 32, 11, 28] y[ 12 ]= 19\n",
      "X[ 13 ]= [33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 32, 11, 28, 19] y[ 13 ]= 15\n",
      "X[ 14 ]= [19, 22, 14, 15, 28, 19, 24, 17, 0, 32, 11, 28, 19, 15] y[ 14 ]= 30\n",
      "X[ 15 ]= [22, 14, 15, 28, 19, 24, 17, 0, 32, 11, 28, 19, 15, 30] y[ 15 ]= 35\n",
      "X[ 16 ]= [14, 15, 28, 19, 24, 17, 0, 32, 11, 28, 19, 15, 30, 35] y[ 16 ]= 0\n",
      "X[ 17 ]= [11, 0, 12, 19, 30, 30, 15, 28, 29, 33, 15, 15, 30, 0] y[ 17 ]= 23\n",
      "X[ 18 ]= [0, 12, 19, 30, 30, 15, 28, 29, 33, 15, 15, 30, 0, 23] y[ 18 ]= 25\n",
      "X[ 19 ]= [12, 19, 30, 30, 15, 28, 29, 33, 15, 15, 30, 0, 23, 25] y[ 19 ]= 23\n"
     ]
    }
   ],
   "source": [
    "print(\"data[0:2]=\", data[0:2])\n",
    "print(\"sentences[0:2]=\", sentences[0:2])\n",
    "\n",
    "for i in range (0,20):\n",
    "    print(\"X[\",i,\"]=\", X[i],\"y[\",i,\"]=\", y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142142, 14, 37)\n"
     ]
    }
   ],
   "source": [
    "#The first row is the X's first row up to 14 character\n",
    "#The second row is the X's first row starting from second character up to 14 character\n",
    "#The third row is the X's first row starting from third character up to 14 character and so on \n",
    "X=np.array(X)\n",
    "X1=np.reshape(X,(X.shape[0],X.shape[1],1))\n",
    "X1=keras.utils.to_categorical(np.array(X1), num_classes=len(char_indices))\n",
    "print(X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142142, 37)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Target Variable\n",
    "y[:10]\n",
    "#Reshapig our label for model\n",
    "y1 = np.array(y)\n",
    "# one hot encode outputs\n",
    "y1 = keras.utils.to_categorical(np.array(y), num_classes=len(char_indices))\n",
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (113713, 14, 37)\n",
      "y_train.shape (113713, 37)\n",
      "X_test.shape (28429, 14, 37)\n",
      "y_test.shape (28429, 37)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.20)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"y_test.shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_4 (SimpleRNN)     (None, 16)                864       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 37)                629       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 37)                0         \n",
      "=================================================================\n",
      "Total params: 1,493\n",
      "Trainable params: 1,493\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#building the model\n",
    "model_RNN2 = Sequential()\n",
    "##model.add(SimpleRNN('number of hidden nodes in each rnn cell', input_shape=(timesteps, data_dim)))\n",
    "model_RNN2.add(SimpleRNN(16, input_shape=(X_train.shape[1], X_train.shape[2]))) \n",
    "model_RNN2.add(Dense(len(char_indices)))\n",
    "model_RNN2.add(Activation('softmax'))\n",
    "model_RNN2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compile network\n",
    "# model_RNN2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# # fit network\n",
    "# model_RNN2.fit(X_train, y_train, epochs=30, verbose=1, validation_data=(X_test, y_test))\n",
    "# model_RNN2.save_weights(\"char_rnn_model_weights_v1.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 113713 samples\n",
      "Epoch 1/2\n",
      "113713/113713 [==============================] - 13s 117us/sample - loss: 1.6414 - accuracy: 0.5102\n",
      "Epoch 2/2\n",
      "113713/113713 [==============================] - 13s 110us/sample - loss: 1.6399 - accuracy: 0.5101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f817a867810>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weightsfile_model_RNN2= \"char_rnn_model_weights_v1.hdf5\"\n",
    "model_RNN2.load_weights(weightsfile_model_RNN2)\n",
    "\n",
    "# compile network\n",
    "model_RNN2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model_RNN2.fit(X_train, y_train, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to prepare test input\n",
    "def prepare_input(in_text):\n",
    "    X1 = np.array([char_indices[i] for i in in_text]).reshape(1,14,1)\n",
    "    X1=keras.utils.to_categorical(np.array(X1), num_classes=len(char_indices))\n",
    "    return(X1)\n",
    "#function to loop our preditions\n",
    "def complete_pred(in_text):\n",
    "    #original_text = in_text\n",
    "    #generated = in_text\n",
    "    completion = ''\n",
    "    while True:\n",
    "        x = prepare_input(in_text)\n",
    "        pred = model_RNN2.predict_classes(x, verbose=0)[0]\n",
    "\n",
    "        next_char = indices_char[pred]\n",
    "\n",
    "        in_text = in_text[1:] + next_char\n",
    "        completion += next_char\n",
    "\n",
    "        if len(completion)> 20 or next_char == ' ':\n",
    "            return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text --> officials say  \n",
      "predicted word --->  of \n",
      "Input text --> how dangerous  \n",
      "predicted output --->  to \n",
      "Input text --> political and  \n",
      "predicted output --->  the \n",
      "Input text --> whatever they  \n",
      "predicted output --->  of \n",
      "Input text --> of particular  \n",
      "predicted output --->  to \n"
     ]
    }
   ],
   "source": [
    "in_text = 'officials say '\n",
    "out_word = complete_pred(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\npredicted word ---> \", out_word)\n",
    "in_text = 'how dangerous '\n",
    "out_word = complete_pred(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\npredicted output ---> \", out_word)\n",
    "in_text = 'political and '\n",
    "out_word = complete_pred(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\npredicted output ---> \", out_word)\n",
    "in_text = 'whatever they '\n",
    "out_word = complete_pred(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\npredicted output ---> \", out_word)\n",
    "in_text = 'of particular '\n",
    "out_word = complete_pred(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\npredicted output ---> \", out_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128)               84992     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 37)                4773      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 37)                0         \n",
      "=================================================================\n",
      "Total params: 89,765\n",
      "Trainable params: 89,765\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#building the model\n",
    "model_LSTM = Sequential()\n",
    "#model1.add(LSTM('number of hidden nodes in each rnn cell', input_shape=(timesteps, data_dim)))\n",
    "model_LSTM.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]))) \n",
    "model_LSTM.add(Dense(len(char_indices)))\n",
    "model_LSTM.add(Activation('softmax'))\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compile network\n",
    "# model_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# # fit network\n",
    "# model_LSTM.fit(X_train, y_train, epochs=30, verbose=1)\n",
    "# model_LSTM.save_weights(\"char_LSTM_model_weights_v1.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 113713 samples\n",
      "Epoch 1/2\n",
      "113713/113713 [==============================] - 37s 326us/sample - loss: 0.7592 - accuracy: 0.7730\n",
      "Epoch 2/2\n",
      "113713/113713 [==============================] - 35s 303us/sample - loss: 0.7217 - accuracy: 0.7794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f816d844590>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weightsfile_model_LSTM= \"char_LSTM_model_weights_v1.hdf5\"\n",
    "model_LSTM.load_weights( weightsfile_model_LSTM)\n",
    "\n",
    "# compile network\n",
    "model_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model_LSTM.fit(X_train, y_train,epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to prepare test input\n",
    "def prepare_input1(in_text):\n",
    "    X1 = np.array([char_indices[i] for i in in_text]).reshape(1,14,1)\n",
    "    X1= keras.utils.to_categorical(np.array(X1), num_classes=len(char_indices))\n",
    "    return(X1)\n",
    "#function to loop our preditions\n",
    "def complete_pred1(in_text):\n",
    "    #original_text = in_text\n",
    "    #generated = in_text\n",
    "    completion = ''\n",
    "    while True:\n",
    "        x = prepare_input1(in_text)\n",
    "        pred = model_LSTM.predict_classes(x, verbose=0)[0]\n",
    "        next_char = indices_char[pred]\n",
    "\n",
    "        in_text = in_text[1:] + next_char\n",
    "        completion += next_char\n",
    "\n",
    "        if len(completion)> 20 or next_char == ' ':\n",
    "            return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text --> the emergence  ; predicted output --->  of \n",
      "Input text --> officials say  ; predicted output --->  they \n",
      "Input text --> and sentenced  ; predicted output --->  to \n",
      "Input text --> a combination  ; predicted output --->  of \n",
      "Input text --> and according  ; predicted output --->  to \n"
     ]
    }
   ],
   "source": [
    "in_text = 'the emergence '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n",
    "in_text = 'officials say '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n",
    "in_text = 'and sentenced '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n",
    "in_text = 'a combination '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n",
    "in_text = 'and according '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few more predictions and Comparions with Standard RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text --> how dangerous  \n",
      "LSTM Prediction --->  it \n",
      "RNN Prediction --->  to \n",
      "\n",
      "\n",
      "Input text --> political and  \n",
      "LSTM Prediction --->  economic \n",
      "RNN Prediction --->  the \n",
      "\n",
      "\n",
      "Input text --> of particular  \n",
      "LSTM Prediction --->  importance \n",
      "RNN Prediction --->  to \n",
      "\n",
      "\n",
      "Input text --> whatever they  \n",
      "LSTM Prediction --->  could \n",
      "RNN Prediction --->  of \n"
     ]
    }
   ],
   "source": [
    "in_text = 'how dangerous '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\nLSTM Prediction ---> \", out_word)\n",
    "out_word1 = complete_pred(in_text)\n",
    "print(\"RNN Prediction ---> \", out_word1)\n",
    "\n",
    "print(\"\\n\")\n",
    "in_text = 'political and '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\nLSTM Prediction ---> \", out_word)\n",
    "out_word1 = complete_pred(in_text)\n",
    "print(\"RNN Prediction ---> \", out_word1)\n",
    "\n",
    "print(\"\\n\")\n",
    "in_text = 'of particular '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\nLSTM Prediction ---> \", out_word)\n",
    "out_word1 = complete_pred(in_text)\n",
    "print(\"RNN Prediction ---> \", out_word1)\n",
    "\n",
    "print(\"\\n\")\n",
    "in_text = 'whatever they '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\nLSTM Prediction ---> \", out_word)\n",
    "out_word1 = complete_pred(in_text)\n",
    "print(\"RNN Prediction ---> \", out_word1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study â€“ Language Translation Project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Go.' 'Va !'\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)']\n",
      " ['Hi.' 'Salut !'\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)']\n",
      " ['Hi.' 'Salut.'\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)']\n",
      " ...\n",
      " [\"Death is something that we're often discouraged to talk about or even think about, but I've realized that preparing for death is one of the most empowering things you can do. Thinking about death clarifies your life.\"\n",
      "  \"La mort est une chose qu'on nous dÃ©courage souvent de discuter ou mÃªme de penser mais j'ai pris conscience que se prÃ©parer Ã  la mort est l'une des choses que nous puissions faire qui nous investit le plus de responsabilitÃ©. RÃ©flÃ©chir Ã  la mort clarifie notre vie.\"\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #1969892 (davearms) & #1969962 (sacredceltic)']\n",
      " ['Since there are usually multiple websites on any given topic, I usually just click the back button when I arrive on any webpage that has pop-up advertising. I just go to the next page found by Google and hope for something less irritating.'\n",
      "  \"Puisqu'il y a de multiples sites web sur chaque sujet, je clique d'habitude sur le bouton retour arriÃ¨re lorsque j'atterris sur n'importe quelle page qui contient des publicitÃ©s surgissantes. Je me rends juste sur la prochaine page proposÃ©e par Google et espÃ¨re tomber sur quelque chose de moins irritant.\"\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #954270 (CK) & #957693 (sacredceltic)']\n",
      " [\"If someone who doesn't know your background says that you sound like a native speaker, it means they probably noticed something about your speaking that made them realize you weren't a native speaker. In other words, you don't really sound like a native speaker.\"\n",
      "  \"Si quelqu'un qui ne connaÃ®t pas vos antÃ©cÃ©dents dit que vous parlez comme un locuteur natif, cela veut dire qu'il a probablement remarquÃ© quelque chose Ã  propos de votre Ã©locution qui lui a fait prendre conscience que vous n'Ãªtes pas un locuteur natif. En d'autres termes, vous ne parlez pas vraiment comme un locuteur natif.\"\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #953936 (CK) & #955961 (sacredceltic)']]\n",
      "Overall pairs 175623\n"
     ]
    }
   ],
   "source": [
    "raw_data= open(Data_path+\"/fra-eng/fra.txt\", mode='rt', encoding='utf-8').read()\n",
    "raw_data=raw_data.strip().split('\\n')\n",
    "raw_data=[i.split('\\t') for i in raw_data]\n",
    "lang1_lang2_data=array(raw_data)\n",
    "print(lang1_lang2_data)\n",
    "print(\"Overall pairs\", len(lang1_lang2_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Go' 'Va '\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)']\n",
      " ['Hi' 'Salut '\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)']\n",
      " ['Hi' 'Salut'\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)']\n",
      " ...\n",
      " ['Death is something that were often discouraged to talk about or even think about but Ive realized that preparing for death is one of the most empowering things you can do Thinking about death clarifies your life'\n",
      "  'La mort est une chose quon nous dÃ©courage souvent de discuter ou mÃªme de penser mais jai pris conscience que se prÃ©parer Ã  la mort est lune des choses que nous puissions faire qui nous investit le plus de responsabilitÃ© RÃ©flÃ©chir Ã  la mort clarifie notre vie'\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #1969892 (davearms) & #1969962 (sacredceltic)']\n",
      " ['Since there are usually multiple websites on any given topic I usually just click the back button when I arrive on any webpage that has popup advertising I just go to the next page found by Google and hope for something less irritating'\n",
      "  'Puisquil y a de multiples sites web sur chaque sujet je clique dhabitude sur le bouton retour arriÃ¨re lorsque jatterris sur nimporte quelle page qui contient des publicitÃ©s surgissantes Je me rends juste sur la prochaine page proposÃ©e par Google et espÃ¨re tomber sur quelque chose de moins irritant'\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #954270 (CK) & #957693 (sacredceltic)']\n",
      " ['If someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker In other words you dont really sound like a native speaker'\n",
      "  'Si quelquun qui ne connaÃ®t pas vos antÃ©cÃ©dents dit que vous parlez comme un locuteur natif cela veut dire quil a probablement remarquÃ© quelque chose Ã  propos de votre Ã©locution qui lui a fait prendre conscience que vous nÃªtes pas un locuteur natif En dautres termes vous ne parlez pas vraiment comme un locuteur natif'\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #953936 (CK) & #955961 (sacredceltic)']]\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "lang1_lang2_data[:,0] = [word.translate(str.maketrans('', '', string.punctuation)) for word in lang1_lang2_data[:,0]]\n",
    "lang1_lang2_data[:,1] = [word.translate(str.maketrans('', '', string.punctuation)) for word in lang1_lang2_data[:,1]]\n",
    "\n",
    "print(lang1_lang2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go' 'va '\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)']\n",
      " ['hi' 'salut '\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)']\n",
      " ['hi' 'salut'\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)']\n",
      " ...\n",
      " ['death is something that were often discouraged to talk about or even think about but ive realized that preparing for death is one of the most empowering things you can do thinking about death clarifies your life'\n",
      "  'la mort est une chose quon nous dÃ©courage souvent de discuter ou mÃªme de penser mais jai pris conscience que se prÃ©parer Ã  la mort est lune des choses que nous puissions faire qui nous investit le plus de responsabilitÃ© rÃ©flÃ©chir Ã  la mort clarifie notre vie'\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #1969892 (davearms) & #1969962 (sacredceltic)']\n",
      " ['since there are usually multiple websites on any given topic i usually just click the back button when i arrive on any webpage that has popup advertising i just go to the next page found by google and hope for something less irritating'\n",
      "  'puisquil y a de multiples sites web sur chaque sujet je clique dhabitude sur le bouton retour arriÃ¨re lorsque jatterris sur nimporte quelle page qui contient des publicitÃ©s surgissantes je me rends juste sur la prochaine page proposÃ©e par google et espÃ¨re tomber sur quelque chose de moins irritant'\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #954270 (CK) & #957693 (sacredceltic)']\n",
      " ['if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker'\n",
      "  'si quelquun qui ne connaÃ®t pas vos antÃ©cÃ©dents dit que vous parlez comme un locuteur natif cela veut dire quil a probablement remarquÃ© quelque chose Ã  propos de votre Ã©locution qui lui a fait prendre conscience que vous nÃªtes pas un locuteur natif en dautres termes vous ne parlez pas vraiment comme un locuteur natif'\n",
      "  'CC-BY 2.0 (France) Attribution: tatoeba.org #953936 (CK) & #955961 (sacredceltic)']]\n"
     ]
    }
   ],
   "source": [
    "## convert text to lowercase\n",
    "for word in range(len(lang1_lang2_data)):\n",
    "    lang1_lang2_data[word,0] = lang1_lang2_data[word,0].lower()\n",
    "    lang1_lang2_data[word,1] = lang1_lang2_data[word,1].lower()\n",
    "print(lang1_lang2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang1_vocab_size 14671\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lang1_lang2_data[:, 0])\n",
    "lang1_tokens=tokenizer\n",
    "lang1_vocab_size = len(lang1_tokens.word_index) + 1\n",
    "print(\"lang1_vocab_size\", lang1_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang2_vocab_size 33321\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lang1_lang2_data[:, 1])\n",
    "lang2_tokens=tokenizer\n",
    "lang2_vocab_size = len(lang2_tokens.word_index) + 1\n",
    "print(\"lang2_vocab_size\", lang2_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test set\n",
    "train, test = train_test_split(lang1_lang2_data, test_size=0.1, random_state = 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (158060, 15)\n",
      "Y_train.shape (158060, 15)\n",
      "X_test.shape (17563, 15)\n",
      "Y_test.shape (17563, 15)\n"
     ]
    }
   ],
   "source": [
    "lang1_seq_length=15\n",
    "lang2_seq_length=15\n",
    "\n",
    "X_train_seq=lang1_tokens.texts_to_sequences(train[:, 0])\n",
    "X_train= pad_sequences(X_train_seq,lang1_seq_length,padding='post')\n",
    "\n",
    "Y_train_seq=lang2_tokens.texts_to_sequences(train[:, 1])\n",
    "Y_train= pad_sequences(Y_train_seq,lang2_seq_length,padding='post')\n",
    "\n",
    "X_test_seq=lang1_tokens.texts_to_sequences(test[:, 0])\n",
    "X_test= pad_sequences(X_test_seq,lang1_seq_length,padding='post')\n",
    "\n",
    "Y_test_seq=lang2_tokens.texts_to_sequences(test[:, 1])\n",
    "Y_test= pad_sequences(Y_test_seq,lang2_seq_length,padding='post')\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"Y_train.shape\",Y_train.shape)\n",
    "print(\"X_test.shape\",X_test.shape)\n",
    "print(\"Y_test.shape\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text data ['i had been studying music in boston before i returned to japan']\n",
      "Numbers sequence [1, 60, 91, 641, 451, 14, 236, 156, 1, 1285, 3, 476]\n",
      "Padded Sequence [   1   60   91  641  451   14  236  156    1 1285    3  476    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Text data\", [train[5, 0]])\n",
    "print('Numbers sequence', X_train_seq[5])\n",
    "print('Padded Sequence', X_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 15, 256)           3755776   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 15, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 15, 128)           131584    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 15, 33321)         4298409   \n",
      "=================================================================\n",
      "Total params: 8,382,889\n",
      "Trainable params: 8,382,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(lang1_vocab_size, 256, input_length=lang1_seq_length, mask_zero=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(RepeatVector(lang2_seq_length))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dense(lang2_vocab_size, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "# history = model.fit(X_train, Y_train.reshape(Y_train.shape[0], Y_train.shape[1], 1),  epochs=1, verbose=1, batch_size=1024)\n",
    "# model.save_weights('Eng_fra_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(Data_path+\"/Pre_trained_models/Eng_fra_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_line_prediction(text1):\n",
    "    \n",
    "    def to_lines(text):\n",
    "          sents = text.strip().split('\\n')\n",
    "          sents = [i.split('\\t') for i in sents]\n",
    "          return sents\n",
    "    small_input = to_lines(text1)\n",
    "    small_input = array(small_input)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    small_input[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in small_input[:,0]]\n",
    "    # convert text to lowercase\n",
    "    for i in range(len(small_input)):\n",
    "        small_input[i,0] = small_input[i,0].lower()\n",
    "\n",
    "    #encode and pad sequences\n",
    "    small_input_seq=lang1_tokens.texts_to_sequences(small_input[0])\n",
    "    small_input= pad_sequences(small_input_seq,lang1_seq_length,padding='post')\n",
    "   \n",
    "\n",
    "    #Load the model\n",
    "    #Eng French Model\n",
    "    #model.load_weights('/content/drive/My Drive/Training/Book/0.Chapters/Chapter12 RNN and LSTM/1.Archives/Eng_fra_model_v2.hdf5')\n",
    "\n",
    "    pred_seq = model.predict_classes(small_input[0:1].reshape((small_input[0:1].shape[0],small_input[0:1].shape[1])))\n",
    "    \n",
    "    def num_to_word(n, tokens):\n",
    "          for word, index in tokens.word_index.items():\n",
    "              if index == n:\n",
    "                  return word\n",
    "          return None\n",
    "\n",
    "    Lang2_text = []\n",
    "    for word_num in pred_seq:\n",
    "          sing_pred = []\n",
    "          for i in range(len(word_num)):\n",
    "                t = num_to_word(word_num[i], lang2_tokens)\n",
    "                if i > 0:\n",
    "                    if (t == num_to_word(word_num[i-1], lang2_tokens)) or (t == None):\n",
    "                        sing_pred.append('')\n",
    "                    else:\n",
    "                        sing_pred.append(t)\n",
    "                else:\n",
    "                      if(t == None):\n",
    "                              sing_pred.append('')\n",
    "                      else:\n",
    "                              sing_pred.append(t) \n",
    "          Lang2_text.append(' '.join(sing_pred))\n",
    "    return(Lang2_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['have a great Good day']  --> ['une  bonne journÃ©e           ']\n",
      "['Do you speak English']  --> ['parlezvous langlais\\u202f             ']\n",
      "['I do not know your language']  --> ['je ne connais pas votre ville         ']\n",
      "['I need help']  --> ['jai besoin de daide           ']\n",
      "['Thank you very much']  --> ['merci beaucoup             ']\n",
      "['Where can I get this']  --> ['oÃ¹ puisje faire            ']\n",
      "['How much does it cost']  --> ['combien  Ã§a coÃ»te\\u202f           ']\n",
      "['Where is the bathroom']  --> ['oÃ¹ est la toilettes de          ']\n",
      "['Where is the ATM']  --> ['oÃ¹ est trouve la           ']\n",
      "['I am a visitor here']  --> ['je suis ici            ']\n",
      "['Excuse me']  --> ['excusezmoi              ']\n",
      "['What do you do for living']  --> ['que  Ã             ']\n",
      "['Here is my passport']  --> ['voici mon passeport            ']\n"
     ]
    }
   ],
   "source": [
    "Input_sentences=[\"have a great Good day\",\n",
    "                 \"Do you speak English\",\n",
    "                 \"I do not know your language\",\n",
    "                 \"I need help\",\n",
    "                 \"Thank you very much\",\n",
    "                 \"Where can I get this\",\n",
    "                 \"How much does it cost\",\n",
    "                 \"Where is the bathroom\",\n",
    "                 \"Where is the ATM\",\n",
    "                 \"I am a visitor here\",\n",
    "                 \"Excuse me\",\n",
    "                 \"What do you do for living\",\n",
    "                 \"Here is my passport\"]\n",
    "\n",
    "for sent in Input_sentences:\n",
    "  print([sent] , \" -->\",one_line_prediction(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

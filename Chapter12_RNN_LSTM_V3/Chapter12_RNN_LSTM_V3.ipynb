{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing dependencies\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from tensorflow.keras.models import Sequential\n",
    "from numpy import array, argmax, random, take\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import RNN, SimpleRNN, LSTM,  Embedding, RepeatVector\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "#For plotting the matplotlib graphs in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data_path=\"D:/Google Drive/Training/Book/0.Chapters/Chapter12 RNN and LSTM/Datasets\"\n",
    "Data_path='Datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data (5351, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "column_names = ['word1', 'word2', 'word3']\n",
    "\n",
    "input_3gram = pd.read_csv(Data_path+ \"/3Gram_love_data.txt\", delimiter='\\t', names=column_names) #Importing csv file with column names\n",
    "print(\"shape of data\", input_3gram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few sample records from data \n",
      "       word1  word2   word3\n",
      "3372   love     to     see\n",
      "3284   love     to     see\n",
      "1324   love    her    very\n",
      "2693   love    the     way\n",
      "387    hate     to    have\n",
      "26     hate     to  bother\n",
      "1881   love     it      if\n",
      "5032  loved  every  minute\n",
      "2067   love   with     the\n",
      "4117   love     it    when\n"
     ]
    }
   ],
   "source": [
    "print(\"Few sample records from data \\n\", input_3gram.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frequency of word1 values \n",
      " love      4327\n",
      "loved      416\n",
      "hate       400\n",
      "hated       80\n",
      "loves       72\n",
      "lovely      24\n",
      "loving      24\n",
      "hates        8\n",
      "Name: word1, dtype: int64\n",
      "\n",
      "Frequency of word2 values \n",
      " to         1866\n",
      "it         1361\n",
      "the         548\n",
      "with        240\n",
      "him         144\n",
      "you         144\n",
      "of          136\n",
      "her         104\n",
      "for          96\n",
      "and          88\n",
      "what         56\n",
      "is           48\n",
      "each         40\n",
      "in           40\n",
      "nothing      32\n",
      "ones         32\n",
      "them         32\n",
      "me           32\n",
      "as           24\n",
      "every        24\n",
      "affair       16\n",
      "being        16\n",
      "my           16\n",
      "more         16\n",
      "that         16\n",
      "going        16\n",
      "got           8\n",
      "letter        8\n",
      "about         8\n",
      "lost          8\n",
      "your          8\n",
      "one           8\n",
      "song          8\n",
      "story         8\n",
      "a             8\n",
      "most          8\n",
      "view          8\n",
      "hearing       8\n",
      "when          8\n",
      "makes         8\n",
      "all           8\n",
      "thy           8\n",
      "this          8\n",
      "on            8\n",
      "husband       8\n",
      "man           8\n",
      "at            8\n",
      "Name: word2, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFrequency of word1 values \\n\", input_3gram[\"word1\"].value_counts())\n",
    "print(\"\\nFrequency of word2 values \\n\", input_3gram[\"word2\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of unique words overall: 139\n",
      "unique words list: ['a' 'able' 'about' 'admit' 'affair' 'affection' 'all' 'and' 'another'\n",
      " 'answer' 'as' 'at' 'be' 'because' 'being' 'better' 'between' 'bother'\n",
      " 'break' 'care' 'cared' 'come' 'concern' 'country' 'cut' 'disappoint' 'do'\n",
      " 'each' 'every' 'fact' 'feel' 'feeling' 'find' 'first' 'for' 'from' 'get'\n",
      " 'go' 'god' 'going' 'got' 'hate' 'hated' 'hates' 'have' 'he' 'hear'\n",
      " 'hearing' 'her' 'here' 'him' 'his' 'husband' 'i' 'idea' 'if' 'in'\n",
      " 'interrupt' 'is' 'it' 'kind' 'know' 'leave' 'letter' 'life' 'like'\n",
      " 'listen' 'look' 'lost' 'lot' 'love' 'loved' 'lovely' 'loves' 'loving'\n",
      " 'make' 'makes' 'man' 'marriage' 'me' 'minute' 'more' 'most' 'much'\n",
      " 'music' 'my' 'nature' 'neighbor' 'not' 'nothing' 'of' 'on' 'one' 'ones'\n",
      " 'or' 'other' 'over' 'play' 'respect' 'say' 'see' 'sit' 'smell' 'so'\n",
      " 'someone' 'song' 'sound' 'story' 'stronger' 'support' 'take' 'talk'\n",
      " 'tell' 'than' 'that' 'the' 'them' 'they' 'think' 'this' 'thought' 'thy'\n",
      " 'to' 'too' 'united' 'use' 'very' 'view' 'watch' 'way' 'we' 'what' 'when'\n",
      " 'wife' 'will' 'with' 'work' 'you' 'your']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Finding our words to create dictionary\n",
    "Here we find unique values in each column and save each of those values .\n",
    "Later which we will take the unique value for the entire appened columns\n",
    "This will be our vocabulary list,which are the unique words in our data file\n",
    "\"\"\"\n",
    "unique_words = []\n",
    "for i in list(input_3gram.columns.values):\n",
    "    for j in pd.unique(input_3gram[i]):\n",
    "        unique_words.append(j)\n",
    "unique_words = np.unique(unique_words)\n",
    "\n",
    "\n",
    "print('Count of unique words overall:', len(unique_words))\n",
    "print('unique words list:', unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_indices dictionary \n",
      " {'a': 0, 'able': 1, 'about': 2, 'admit': 3, 'affair': 4, 'affection': 5, 'all': 6, 'and': 7, 'another': 8, 'answer': 9, 'as': 10, 'at': 11, 'be': 12, 'because': 13, 'being': 14, 'better': 15, 'between': 16, 'bother': 17, 'break': 18, 'care': 19, 'cared': 20, 'come': 21, 'concern': 22, 'country': 23, 'cut': 24, 'disappoint': 25, 'do': 26, 'each': 27, 'every': 28, 'fact': 29, 'feel': 30, 'feeling': 31, 'find': 32, 'first': 33, 'for': 34, 'from': 35, 'get': 36, 'go': 37, 'god': 38, 'going': 39, 'got': 40, 'hate': 41, 'hated': 42, 'hates': 43, 'have': 44, 'he': 45, 'hear': 46, 'hearing': 47, 'her': 48, 'here': 49, 'him': 50, 'his': 51, 'husband': 52, 'i': 53, 'idea': 54, 'if': 55, 'in': 56, 'interrupt': 57, 'is': 58, 'it': 59, 'kind': 60, 'know': 61, 'leave': 62, 'letter': 63, 'life': 64, 'like': 65, 'listen': 66, 'look': 67, 'lost': 68, 'lot': 69, 'love': 70, 'loved': 71, 'lovely': 72, 'loves': 73, 'loving': 74, 'make': 75, 'makes': 76, 'man': 77, 'marriage': 78, 'me': 79, 'minute': 80, 'more': 81, 'most': 82, 'much': 83, 'music': 84, 'my': 85, 'nature': 86, 'neighbor': 87, 'not': 88, 'nothing': 89, 'of': 90, 'on': 91, 'one': 92, 'ones': 93, 'or': 94, 'other': 95, 'over': 96, 'play': 97, 'respect': 98, 'say': 99, 'see': 100, 'sit': 101, 'smell': 102, 'so': 103, 'someone': 104, 'song': 105, 'sound': 106, 'story': 107, 'stronger': 108, 'support': 109, 'take': 110, 'talk': 111, 'tell': 112, 'than': 113, 'that': 114, 'the': 115, 'them': 116, 'they': 117, 'think': 118, 'this': 119, 'thought': 120, 'thy': 121, 'to': 122, 'too': 123, 'united': 124, 'use': 125, 'very': 126, 'view': 127, 'watch': 128, 'way': 129, 'we': 130, 'what': 131, 'when': 132, 'wife': 133, 'will': 134, 'with': 135, 'work': 136, 'you': 137, 'your': 138}\n",
      "word_indices.keys \n",
      " dict_keys(['a', 'able', 'about', 'admit', 'affair', 'affection', 'all', 'and', 'another', 'answer', 'as', 'at', 'be', 'because', 'being', 'better', 'between', 'bother', 'break', 'care', 'cared', 'come', 'concern', 'country', 'cut', 'disappoint', 'do', 'each', 'every', 'fact', 'feel', 'feeling', 'find', 'first', 'for', 'from', 'get', 'go', 'god', 'going', 'got', 'hate', 'hated', 'hates', 'have', 'he', 'hear', 'hearing', 'her', 'here', 'him', 'his', 'husband', 'i', 'idea', 'if', 'in', 'interrupt', 'is', 'it', 'kind', 'know', 'leave', 'letter', 'life', 'like', 'listen', 'look', 'lost', 'lot', 'love', 'loved', 'lovely', 'loves', 'loving', 'make', 'makes', 'man', 'marriage', 'me', 'minute', 'more', 'most', 'much', 'music', 'my', 'nature', 'neighbor', 'not', 'nothing', 'of', 'on', 'one', 'ones', 'or', 'other', 'over', 'play', 'respect', 'say', 'see', 'sit', 'smell', 'so', 'someone', 'song', 'sound', 'story', 'stronger', 'support', 'take', 'talk', 'tell', 'than', 'that', 'the', 'them', 'they', 'think', 'this', 'thought', 'thy', 'to', 'too', 'united', 'use', 'very', 'view', 'watch', 'way', 'we', 'what', 'when', 'wife', 'will', 'with', 'work', 'you', 'your'])\n",
      "word_indices.values \n",
      " dict_values([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138])\n",
      "\n",
      " ########################################\n",
      "\n",
      "indices_words dictionary \n",
      " {0: 'a', 1: 'able', 2: 'about', 3: 'admit', 4: 'affair', 5: 'affection', 6: 'all', 7: 'and', 8: 'another', 9: 'answer', 10: 'as', 11: 'at', 12: 'be', 13: 'because', 14: 'being', 15: 'better', 16: 'between', 17: 'bother', 18: 'break', 19: 'care', 20: 'cared', 21: 'come', 22: 'concern', 23: 'country', 24: 'cut', 25: 'disappoint', 26: 'do', 27: 'each', 28: 'every', 29: 'fact', 30: 'feel', 31: 'feeling', 32: 'find', 33: 'first', 34: 'for', 35: 'from', 36: 'get', 37: 'go', 38: 'god', 39: 'going', 40: 'got', 41: 'hate', 42: 'hated', 43: 'hates', 44: 'have', 45: 'he', 46: 'hear', 47: 'hearing', 48: 'her', 49: 'here', 50: 'him', 51: 'his', 52: 'husband', 53: 'i', 54: 'idea', 55: 'if', 56: 'in', 57: 'interrupt', 58: 'is', 59: 'it', 60: 'kind', 61: 'know', 62: 'leave', 63: 'letter', 64: 'life', 65: 'like', 66: 'listen', 67: 'look', 68: 'lost', 69: 'lot', 70: 'love', 71: 'loved', 72: 'lovely', 73: 'loves', 74: 'loving', 75: 'make', 76: 'makes', 77: 'man', 78: 'marriage', 79: 'me', 80: 'minute', 81: 'more', 82: 'most', 83: 'much', 84: 'music', 85: 'my', 86: 'nature', 87: 'neighbor', 88: 'not', 89: 'nothing', 90: 'of', 91: 'on', 92: 'one', 93: 'ones', 94: 'or', 95: 'other', 96: 'over', 97: 'play', 98: 'respect', 99: 'say', 100: 'see', 101: 'sit', 102: 'smell', 103: 'so', 104: 'someone', 105: 'song', 106: 'sound', 107: 'story', 108: 'stronger', 109: 'support', 110: 'take', 111: 'talk', 112: 'tell', 113: 'than', 114: 'that', 115: 'the', 116: 'them', 117: 'they', 118: 'think', 119: 'this', 120: 'thought', 121: 'thy', 122: 'to', 123: 'too', 124: 'united', 125: 'use', 126: 'very', 127: 'view', 128: 'watch', 129: 'way', 130: 'we', 131: 'what', 132: 'when', 133: 'wife', 134: 'will', 135: 'with', 136: 'work', 137: 'you', 138: 'your'}\n",
      "indices_words keys \n",
      " dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138])\n",
      "indices_words values \n",
      " dict_values(['a', 'able', 'about', 'admit', 'affair', 'affection', 'all', 'and', 'another', 'answer', 'as', 'at', 'be', 'because', 'being', 'better', 'between', 'bother', 'break', 'care', 'cared', 'come', 'concern', 'country', 'cut', 'disappoint', 'do', 'each', 'every', 'fact', 'feel', 'feeling', 'find', 'first', 'for', 'from', 'get', 'go', 'god', 'going', 'got', 'hate', 'hated', 'hates', 'have', 'he', 'hear', 'hearing', 'her', 'here', 'him', 'his', 'husband', 'i', 'idea', 'if', 'in', 'interrupt', 'is', 'it', 'kind', 'know', 'leave', 'letter', 'life', 'like', 'listen', 'look', 'lost', 'lot', 'love', 'loved', 'lovely', 'loves', 'loving', 'make', 'makes', 'man', 'marriage', 'me', 'minute', 'more', 'most', 'much', 'music', 'my', 'nature', 'neighbor', 'not', 'nothing', 'of', 'on', 'one', 'ones', 'or', 'other', 'over', 'play', 'respect', 'say', 'see', 'sit', 'smell', 'so', 'someone', 'song', 'sound', 'story', 'stronger', 'support', 'take', 'talk', 'tell', 'than', 'that', 'the', 'them', 'they', 'think', 'this', 'thought', 'thy', 'to', 'too', 'united', 'use', 'very', 'view', 'watch', 'way', 'we', 'what', 'when', 'wife', 'will', 'with', 'work', 'you', 'your'])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "creating our word:indice pair dictionary and inverse\n",
    "Here will be creating two dictonary values\n",
    "word_indices : This contains each words mapped to an unique digit \n",
    "indices_words : This contains each digits mapped to a word in the same sequence as word_indices \n",
    "\"\"\"\n",
    "word_indices = dict((w, i) for i, w in enumerate(unique_words))\n",
    "indices_words = dict((i, w) for i, w in enumerate(unique_words))\n",
    "\n",
    "print(\"word_indices dictionary \\n\",word_indices)\n",
    "print(\"word_indices.keys \\n\", word_indices.keys())\n",
    "print(\"word_indices.values \\n\", word_indices.values())\n",
    "print(\"\\n ########################################\\n\")\n",
    "print(\"indices_words dictionary \\n\", indices_words)\n",
    "print(\"indices_words keys \\n\",indices_words.keys())\n",
    "print(\"indices_words values \\n\",indices_words.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word1_onehot shape is  (5351, 139)\n"
     ]
    }
   ],
   "source": [
    "### Onehot encoding of word1\n",
    "word1 = input_3gram['word1'].map(word_indices)\n",
    "word1_onehot = keras.utils.to_categorical(np.array(word1), num_classes=len(word_indices))\n",
    "print(\"word1_onehot shape is \",word1_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word in row 0 is -->hate\n",
      "The one hot encoded version of the word in row 0 is \n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "The word in row 500 is --> love\n",
      "The one hot encoded version of the word in row 500 is \n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Lets take example of two different words\n",
    "print(\"The word in row 0 is -->\"+input_3gram['word1'][0])\n",
    "print(\"The one hot encoded version of the word in row 0 is \\n\",word1_onehot[0])\n",
    "\n",
    "print(\"\\nThe word in row 500 is --> \"+input_3gram['word1'][500])\n",
    "print(\"The one hot encoded version of the word in row 500 is \\n\",word1_onehot[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2_onehot shape is  (5351, 139)\n",
      "word3_onehot shape is  (5351, 139)\n"
     ]
    }
   ],
   "source": [
    "##one hot encoding for word2 and word3 \n",
    "word2 = input_3gram['word2'].map(word_indices)\n",
    "word2_onehot = keras.utils.to_categorical(np.array(word2), num_classes=len(word_indices))\n",
    "print(\"word2_onehot shape is \",word2_onehot.shape)\n",
    "\n",
    "word3 = input_3gram['word3'].map(word_indices)\n",
    "word3_onehot = keras.utils.to_categorical(np.array(word3), num_classes=len(word_indices))\n",
    "print(\"word3_onehot shape is \",word3_onehot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                1400      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 139)               1529      \n",
      "=================================================================\n",
      "Total params: 2,929\n",
      "Trainable params: 2,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ANN_model1 = Sequential()\n",
    "ANN_model1.add(Dense(10, input_dim=word1_onehot.shape[1], activation='sigmoid'))\n",
    "ANN_model1.add(Dense(word2_onehot.shape[1] ,activation='softmax'))\n",
    "ANN_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5351 samples\n",
      "Epoch 1/20\n",
      "5351/5351 [==============================] - 1s 131us/sample - loss: 0.0391 - accuracy: 0.9928\n",
      "Epoch 2/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0321 - accuracy: 0.9928\n",
      "Epoch 3/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0265 - accuracy: 0.9928\n",
      "Epoch 4/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0238 - accuracy: 0.9928\n",
      "Epoch 5/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0231 - accuracy: 0.9928\n",
      "Epoch 6/20\n",
      "5351/5351 [==============================] - 0s 23us/sample - loss: 0.0228 - accuracy: 0.9928\n",
      "Epoch 7/20\n",
      "5351/5351 [==============================] - 0s 21us/sample - loss: 0.0227 - accuracy: 0.9928\n",
      "Epoch 8/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0226 - accuracy: 0.9928\n",
      "Epoch 9/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0225 - accuracy: 0.9928\n",
      "Epoch 10/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0225 - accuracy: 0.9928\n",
      "Epoch 11/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0224 - accuracy: 0.9928\n",
      "Epoch 12/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0224 - accuracy: 0.9928\n",
      "Epoch 13/20\n",
      "5351/5351 [==============================] - 0s 25us/sample - loss: 0.0224 - accuracy: 0.9928\n",
      "Epoch 14/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0223 - accuracy: 0.9928\n",
      "Epoch 15/20\n",
      "5351/5351 [==============================] - 0s 21us/sample - loss: 0.0223 - accuracy: 0.9928\n",
      "Epoch 16/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0223 - accuracy: 0.9928\n",
      "Epoch 17/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0223 - accuracy: 0.9928\n",
      "Epoch 18/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0223 - accuracy: 0.9928\n",
      "Epoch 19/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0222 - accuracy: 0.9928\n",
      "Epoch 20/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0222 - accuracy: 0.9928\n"
     ]
    }
   ],
   "source": [
    "ANN_model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train model\n",
    "history = ANN_model1.fit(word1_onehot, word2_onehot, epochs=20, batch_size=50,  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will see what the 1st hidden layer output representation of the data  \n",
    "# to predict the hidden layer activations, \n",
    "# let's rewrite first layer of our model and give it the weights from fully trained previous model\n",
    "model1_hidden = Sequential()\n",
    "model1_hidden.add(Dense(10, input_dim=word1_onehot.shape[1], weights=ANN_model1.layers[0].get_weights()))\n",
    "model1_hidden.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hidden layer output for every record - Shape of it \n",
      " (5351, 10)\n",
      "Few five records from hidden layer \n",
      " [[0.8502294  0.7549435  0.8281398  0.81124496 0.7960458  0.803674\n",
      "  0.8720081  0.66531056 0.8824079  0.8776932 ]\n",
      " [0.8502294  0.7549435  0.8281398  0.81124496 0.7960458  0.803674\n",
      "  0.8720081  0.66531056 0.8824079  0.8776932 ]\n",
      " [0.8502294  0.7549435  0.8281398  0.81124496 0.7960458  0.803674\n",
      "  0.8720081  0.66531056 0.8824079  0.8776932 ]\n",
      " [0.8502294  0.7549435  0.8281398  0.81124496 0.7960458  0.803674\n",
      "  0.8720081  0.66531056 0.8824079  0.8776932 ]\n",
      " [0.8502294  0.7549435  0.8281398  0.81124496 0.7960458  0.803674\n",
      "  0.8720081  0.66531056 0.8824079  0.8776932 ]]\n"
     ]
    }
   ],
   "source": [
    "# Getting the hidden layer activations\n",
    "model1_hidden_output = model1_hidden.predict(word1_onehot)\n",
    "#peak into our hidden layer activations\n",
    "print(\"The hidden layer output for every record - Shape of it \\n\", model1_hidden_output.shape)\n",
    "print(\"Few five records from hidden layer \\n\",model1_hidden_output[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2_hidden_append Shape (5351, 149)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We append the input words of the words2 column in the output of the h1 layer,this gives us the combined input representation\n",
    "\"\"\"\n",
    "word2_hidden_append = np.append(model1_hidden_output,word2_onehot, axis=1)\n",
    "print(\"word2_hidden_append Shape\", word2_hidden_append.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10)                1500      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 139)               1529      \n",
      "=================================================================\n",
      "Total params: 3,029\n",
      "Trainable params: 3,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ANN_model2 = Sequential()\n",
    "ANN_model2.add(Dense(10, input_dim=word2_hidden_append.shape[1], activation='sigmoid'))\n",
    "ANN_model2.add(Dense(word3_onehot.shape[1], activation='softmax'))\n",
    "ANN_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5351 samples\n",
      "Epoch 1/20\n",
      "5351/5351 [==============================] - 0s 92us/sample - loss: 0.0401 - accuracy: 0.9928\n",
      "Epoch 2/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0344 - accuracy: 0.9928\n",
      "Epoch 3/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0309 - accuracy: 0.9928\n",
      "Epoch 4/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0302 - accuracy: 0.9928\n",
      "Epoch 5/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0301 - accuracy: 0.9928\n",
      "Epoch 6/20\n",
      "5351/5351 [==============================] - 0s 23us/sample - loss: 0.0299 - accuracy: 0.9928\n",
      "Epoch 7/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0296 - accuracy: 0.9928\n",
      "Epoch 8/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0292 - accuracy: 0.9928\n",
      "Epoch 9/20\n",
      "5351/5351 [==============================] - 0s 23us/sample - loss: 0.0287 - accuracy: 0.9928\n",
      "Epoch 10/20\n",
      "5351/5351 [==============================] - 0s 23us/sample - loss: 0.0281 - accuracy: 0.9928\n",
      "Epoch 11/20\n",
      "5351/5351 [==============================] - 0s 23us/sample - loss: 0.0275 - accuracy: 0.9934\n",
      "Epoch 12/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0268 - accuracy: 0.9945\n",
      "Epoch 13/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0261 - accuracy: 0.9945\n",
      "Epoch 14/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0254 - accuracy: 0.9945\n",
      "Epoch 15/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0249 - accuracy: 0.9945\n",
      "Epoch 16/20\n",
      "5351/5351 [==============================] - 0s 23us/sample - loss: 0.0243 - accuracy: 0.9945\n",
      "Epoch 17/20\n",
      "5351/5351 [==============================] - 0s 26us/sample - loss: 0.0239 - accuracy: 0.9945\n",
      "Epoch 18/20\n",
      "5351/5351 [==============================] - 0s 26us/sample - loss: 0.0235 - accuracy: 0.9945\n",
      "Epoch 19/20\n",
      "5351/5351 [==============================] - 0s 21us/sample - loss: 0.0231 - accuracy: 0.9947\n",
      "Epoch 20/20\n",
      "5351/5351 [==============================] - 0s 22us/sample - loss: 0.0227 - accuracy: 0.9950\n"
     ]
    }
   ],
   "source": [
    "ANN_model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Train model\n",
    "history = ANN_model2.fit(word2_hidden_append, word3_onehot, epochs=20, batch_size=50,  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A predict function that takes input word1 and word2; and predict word3 \n",
    "#1. take the input word , and represent them using digits from the word_indices dictonary values\n",
    "#2. getting the intermediate hidden nodes for word1\n",
    "#3. appending hidden activations with word2 as final test set\n",
    "#4. prediction on this test set\n",
    "def two_step_pred(words_in):\n",
    "\n",
    "    index_input=word_indices[words_in[0]]\n",
    "    indices_in = keras.utils.to_categorical(index_input, num_classes=len(word_indices))\n",
    "    indices_in=indices_in.reshape(1,len(word_indices))\n",
    "    h1_test = model1_hidden.predict(indices_in) # getting our intermediate hidden activations from model1h\n",
    "    \n",
    "    \n",
    "    index_input2=word_indices[words_in[1]]\n",
    "    indices_in2 = keras.utils.to_categorical(index_input2, num_classes=len(word_indices))\n",
    "    indices_in2= indices_in2.reshape(1,len(word_indices))\n",
    "    X2_test = np.append(h1_test, indices_in2, axis=1) #preparing final test data by appending hidden with word2\n",
    "    \n",
    "    yhat = ANN_model2.predict_classes(X2_test) #predicting final output from model2\n",
    "    \n",
    "    print(\"Input words --> \", words_in)\n",
    "    print(\"Predicted word --> \", indices_words[yhat[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input words -->  ['love', 'it']\n",
      "Predicted word -->  when\n",
      "Input words -->  ['love', 'to']\n",
      "Predicted word -->  see\n",
      "Input words -->  ['love', 'the']\n",
      "Predicted word -->  way\n"
     ]
    }
   ],
   "source": [
    "two_step_pred(['love', 'it'])\n",
    "two_step_pred(['love', 'to'])\n",
    "two_step_pred(['love', 'the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, 4)                 24        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 12        \n",
      "=================================================================\n",
      "Total params: 36\n",
      "Trainable params: 36\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(4, use_bias=False, input_shape=(2,2)))\n",
    "model.add(Dense(3, use_bias=False, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 43\n",
      "Trainable params: 43\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(4, input_shape=(2,2)))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 43\n",
      "Trainable params: 43\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(4, input_shape=(4,2)))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word prediction using RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word1_word2_onehot shape (5351, 2, 139)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shuang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "word1_word2 = input_3gram[['word1','word2']]\n",
    "for i in list(word1_word2.columns.values):\n",
    "    word1_word2[i] = word1_word2[i].map(word_indices)\n",
    "\n",
    "word1_word2=np.array(word1_word2)\n",
    "#The same data is reshaped with similar structure but appended with 1 value to make it 3d array\n",
    "word1_word2=np.reshape(word1_word2,(word1_word2.shape[0],2,1))\n",
    "word1_word2_onehot = keras.utils.to_categorical(np.array(word1_word2), num_classes=len(word_indices))\n",
    "print(\"word1_word2_onehot shape\", word1_word2_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time steps 2\n",
      "Input nodes 139\n",
      "output nodes 139\n"
     ]
    }
   ],
   "source": [
    "print(\"time steps\" , word1_word2_onehot.shape[1])\n",
    "print(\"Input nodes\" , word1_word2_onehot.shape[2])\n",
    "print(\"output nodes\" , word3_onehot.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_3 (SimpleRNN)     (None, 30)                5100      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 139)               4309      \n",
      "=================================================================\n",
      "Total params: 9,409\n",
      "Trainable params: 9,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_rnn = Sequential()\n",
    "#model.add(SimpleRNN('number of hidden nodes in each rnn cell', input_shape=(timesteps, input_data_dim)))\n",
    "model_rnn.add(SimpleRNN(30, input_shape=(word1_word2_onehot.shape[1],word1_word2_onehot.shape[2]))) \n",
    "model_rnn.add(Dense(word3_onehot.shape[1], activation='softmax'))\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5351 samples\n",
      "Epoch 1/20\n",
      "5351/5351 [==============================] - 1s 189us/sample - loss: 3.7526 - accuracy: 0.4016\n",
      "Epoch 2/20\n",
      "5351/5351 [==============================] - 0s 46us/sample - loss: 2.7391 - accuracy: 0.4947\n",
      "Epoch 3/20\n",
      "5351/5351 [==============================] - 0s 47us/sample - loss: 2.3301 - accuracy: 0.5352\n",
      "Epoch 4/20\n",
      "5351/5351 [==============================] - 0s 47us/sample - loss: 2.0661 - accuracy: 0.5666\n",
      "Epoch 5/20\n",
      "5351/5351 [==============================] - 0s 45us/sample - loss: 1.8881 - accuracy: 0.5842\n",
      "Epoch 6/20\n",
      "5351/5351 [==============================] - 0s 46us/sample - loss: 1.7518 - accuracy: 0.6031\n",
      "Epoch 7/20\n",
      "5351/5351 [==============================] - 0s 47us/sample - loss: 1.6412 - accuracy: 0.6227\n",
      "Epoch 8/20\n",
      "5351/5351 [==============================] - 0s 45us/sample - loss: 1.5513 - accuracy: 0.6416\n",
      "Epoch 9/20\n",
      "5351/5351 [==============================] - 0s 46us/sample - loss: 1.4775 - accuracy: 0.6515\n",
      "Epoch 10/20\n",
      "5351/5351 [==============================] - 0s 46us/sample - loss: 1.4168 - accuracy: 0.6580\n",
      "Epoch 11/20\n",
      "5351/5351 [==============================] - 0s 47us/sample - loss: 1.3679 - accuracy: 0.6610\n",
      "Epoch 12/20\n",
      "5351/5351 [==============================] - 0s 47us/sample - loss: 1.3280 - accuracy: 0.6647\n",
      "Epoch 13/20\n",
      "5351/5351 [==============================] - 0s 45us/sample - loss: 1.2962 - accuracy: 0.6632\n",
      "Epoch 14/20\n",
      "5351/5351 [==============================] - 0s 46us/sample - loss: 1.2709 - accuracy: 0.6634\n",
      "Epoch 15/20\n",
      "5351/5351 [==============================] - 0s 46us/sample - loss: 1.2514 - accuracy: 0.6638\n",
      "Epoch 16/20\n",
      "5351/5351 [==============================] - 0s 48us/sample - loss: 1.2352 - accuracy: 0.6677\n",
      "Epoch 17/20\n",
      "5351/5351 [==============================] - 0s 46us/sample - loss: 1.2217 - accuracy: 0.6632\n",
      "Epoch 18/20\n",
      "5351/5351 [==============================] - 0s 46us/sample - loss: 1.2124 - accuracy: 0.6640\n",
      "Epoch 19/20\n",
      "5351/5351 [==============================] - 0s 46us/sample - loss: 1.2039 - accuracy: 0.6640\n",
      "Epoch 20/20\n",
      "5351/5351 [==============================] - 0s 51us/sample - loss: 1.1965 - accuracy: 0.6623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa493c82410>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile network\n",
    "model_rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model_rnn.fit(word1_word2_onehot, word3_onehot, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_word_pred(in_text):\n",
    "    print(\"Input is - \" , in_text)\n",
    "    encoded = [word_indices[i] for i in in_text]\n",
    "    encoded = np.array(encoded).reshape(1,2,1)\n",
    "    encoded =keras.utils.to_categorical(np.array(encoded), num_classes=len(word_indices))\n",
    "    ypred = model_rnn.predict_classes(encoded, verbose=0)[0]\n",
    "    print(\"Output is --> \" ,indices_words[ypred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input is -  ['love', 'it']\n",
      "Output is -->  when\n",
      "Input is -  ['love', 'to']\n",
      "Output is -->  see\n",
      "Input is -  ['love', 'the']\n",
      "Output is -->  way\n"
     ]
    }
   ],
   "source": [
    "rnn_word_pred(['love', 'it'])\n",
    "rnn_word_pred(['love', 'to'])\n",
    "rnn_word_pred(['love', 'the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN for Long Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "a,combination,of\n",
      "\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,according,to\n",
      "and,addresses,of\n",
      "and,adherence,to\n",
      "and,advocates,for\n",
      "and,aerospace,engineering\n",
      "and,americans,do\n",
      "and,analyzing,the\n",
      "and,announced,he\n",
      "and,announced,he\n",
      "and,announced,plans\n",
      "and,announced,that\n",
      "and,announced,that\n",
      "and,annou\n"
     ]
    }
   ],
   "source": [
    "longseq_3gram = open(Data_path+'/Long_sequence_3gram.csv').read().lower()\n",
    "print(longseq_3gram[495:801])\n",
    "print(longseq_3gram[30615:31000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "a combination of\n",
      "\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and according to\n",
      "and addresses \n"
     ]
    }
   ],
   "source": [
    "#Replace comma with space\n",
    "longseq_3gram1= longseq_3gram.replace(',',' ').replace('\\r','')\n",
    "print(longseq_3gram1[495:750])\n",
    "print(longseq_3gram1[30615:30800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Characters in the text \n",
      "  ['\\n', ' ', \"'\", '(', '-', '.', '/', '0', '1', '3', '7', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      " Character after removing newline symbol '\\n' [' ', \"'\", '(', '-', '.', '/', '0', '1', '3', '7', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      " overall chars count 37\n"
     ]
    }
   ],
   "source": [
    "#Unique characters in our dataset we then sort it\n",
    "chars = sorted(list(set(longseq_3gram1)))\n",
    "print(\"Unique Characters in the text \\n \",chars)\n",
    "#\\n is character string for new line, we dont need that in our dictionary of chars\n",
    "chars.remove('\\n')\n",
    "print(\"\\n Character after removing newline symbol \\'/n\\'\",chars)\n",
    "print(\"\\n overall chars count\", len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "characters to indices dictionary\n",
      " {' ': 0, \"'\": 1, '(': 2, '-': 3, '.': 4, '/': 5, '0': 6, '1': 7, '3': 8, '7': 9, '9': 10, 'a': 11, 'b': 12, 'c': 13, 'd': 14, 'e': 15, 'f': 16, 'g': 17, 'h': 18, 'i': 19, 'j': 20, 'k': 21, 'l': 22, 'm': 23, 'n': 24, 'o': 25, 'p': 26, 'q': 27, 'r': 28, 's': 29, 't': 30, 'u': 31, 'v': 32, 'w': 33, 'x': 34, 'y': 35, 'z': 36}\n",
      "indices to char dictionary\n",
      " {0: ' ', 1: \"'\", 2: '(', 3: '-', 4: '.', 5: '/', 6: '0', 7: '1', 8: '3', 9: '7', 10: '9', 11: 'a', 12: 'b', 13: 'c', 14: 'd', 15: 'e', 16: 'f', 17: 'g', 18: 'h', 19: 'i', 20: 'j', 21: 'k', 22: 'l', 23: 'm', 24: 'n', 25: 'o', 26: 'p', 27: 'q', 28: 'r', 29: 's', 30: 't', 31: 'u', 32: 'v', 33: 'w', 34: 'x', 35: 'y', 36: 'z'}\n",
      "unique chars:  {37}\n"
     ]
    }
   ],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "print(\"characters to indices dictionary\\n\", char_indices)\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print(\"indices to char dictionary\\n\", indices_char)\n",
    "print('unique chars: ', {len(chars)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Char to index on full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a bewildering array  [11, 0, 12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 11, 28, 28, 11, 35, 0]\n",
      "a celebration of  [11, 0, 12, 15, 24, 15, 16, 19, 13, 19, 11, 28, 35, 0, 25, 16, 0]\n",
      "a co-director of  [11, 0, 12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 32, 11, 28, 19, 15, 30, 35, 0]\n",
      "a declaration of  [11, 0, 12, 19, 30, 30, 15, 28, 29, 33, 15, 15, 30, 0, 23, 25, 23, 15, 24, 30, 0]\n",
      "a significant risk  [11, 0, 29, 19, 17, 24, 19, 16, 19, 13, 11, 24, 30, 0, 28, 19, 29, 21, 0]\n",
      "been designed as  [12, 15, 15, 24, 0, 14, 15, 29, 19, 17, 24, 15, 14, 0, 11, 29, 0]\n",
      "from anywhere on  [16, 28, 25, 23, 0, 11, 24, 35, 33, 18, 15, 28, 15, 0, 25, 24, 0]\n",
      "Number of sentences  30307\n"
     ]
    }
   ],
   "source": [
    "data = longseq_3gram1.splitlines()\n",
    "##Adding a space at the end\n",
    "data = [i+' ' for i in data]\n",
    "\n",
    "##mapping our data into numbers\n",
    "sentences = [[char_indices[j] for j in i] for i in data ]\n",
    "print(data[0], sentences[0])\n",
    "print(data[10], sentences[1])\n",
    "print(data[20], sentences[2])\n",
    "print(data[100], sentences[3])\n",
    "print(data[400], sentences[400])\n",
    "print(data[4000], sentences[4000])\n",
    "print(data[9000], sentences[9000])\n",
    "##Number of sentences\n",
    "print(\"Number of sentences \", len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the sentences to RNN friendly data which can be used to generate new sequences\n",
    "\n",
    "**Take one sentence, iterate through it till the length of sentence is reached:**\n",
    "\n",
    "* **Step 1** 0:0+14: X; and 14th position: y >> observation 1\n",
    "* **Step 2** 1:14: X; and 15th position: y >> observation 2\n",
    "* **Step 3** â€¦we do this till the length of sentence is reached\n",
    "\n",
    "Take next sentence and repeat the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142142, 142142)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since all the sentences may not be of same length,it is neccessary to make them consistent when passing to keras\n",
    "#We select a sequence length\n",
    "Seq_ln = 14\n",
    "X = []\n",
    "y = []\n",
    "for i in sentences:\n",
    "    for j in range(len(i)-Seq_ln):\n",
    "        X.append(i[j:j+Seq_ln])\n",
    "        y.append(i[j+Seq_ln])\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[0:2]= ['a bewildering array ', 'a beneficiary of ']\n",
      "sentences[0:2]= [[11, 0, 12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 11, 28, 28, 11, 35, 0], [11, 0, 12, 15, 24, 15, 16, 19, 13, 19, 11, 28, 35, 0, 25, 16, 0]]\n",
      "X[ 0 ]= [11, 0, 12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0] y[ 0 ]= 11\n",
      "X[ 1 ]= [0, 12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 11] y[ 1 ]= 28\n",
      "X[ 2 ]= [12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 11, 28] y[ 2 ]= 28\n",
      "X[ 3 ]= [15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 11, 28, 28] y[ 3 ]= 11\n",
      "X[ 4 ]= [33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 11, 28, 28, 11] y[ 4 ]= 35\n",
      "X[ 5 ]= [19, 22, 14, 15, 28, 19, 24, 17, 0, 11, 28, 28, 11, 35] y[ 5 ]= 0\n",
      "X[ 6 ]= [11, 0, 12, 15, 24, 15, 16, 19, 13, 19, 11, 28, 35, 0] y[ 6 ]= 25\n",
      "X[ 7 ]= [0, 12, 15, 24, 15, 16, 19, 13, 19, 11, 28, 35, 0, 25] y[ 7 ]= 16\n",
      "X[ 8 ]= [12, 15, 24, 15, 16, 19, 13, 19, 11, 28, 35, 0, 25, 16] y[ 8 ]= 0\n",
      "X[ 9 ]= [11, 0, 12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0] y[ 9 ]= 32\n",
      "X[ 10 ]= [0, 12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 32] y[ 10 ]= 11\n",
      "X[ 11 ]= [12, 15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 32, 11] y[ 11 ]= 28\n",
      "X[ 12 ]= [15, 33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 32, 11, 28] y[ 12 ]= 19\n",
      "X[ 13 ]= [33, 19, 22, 14, 15, 28, 19, 24, 17, 0, 32, 11, 28, 19] y[ 13 ]= 15\n",
      "X[ 14 ]= [19, 22, 14, 15, 28, 19, 24, 17, 0, 32, 11, 28, 19, 15] y[ 14 ]= 30\n",
      "X[ 15 ]= [22, 14, 15, 28, 19, 24, 17, 0, 32, 11, 28, 19, 15, 30] y[ 15 ]= 35\n",
      "X[ 16 ]= [14, 15, 28, 19, 24, 17, 0, 32, 11, 28, 19, 15, 30, 35] y[ 16 ]= 0\n",
      "X[ 17 ]= [11, 0, 12, 19, 30, 30, 15, 28, 29, 33, 15, 15, 30, 0] y[ 17 ]= 23\n",
      "X[ 18 ]= [0, 12, 19, 30, 30, 15, 28, 29, 33, 15, 15, 30, 0, 23] y[ 18 ]= 25\n",
      "X[ 19 ]= [12, 19, 30, 30, 15, 28, 29, 33, 15, 15, 30, 0, 23, 25] y[ 19 ]= 23\n"
     ]
    }
   ],
   "source": [
    "print(\"data[0:2]=\", data[0:2])\n",
    "print(\"sentences[0:2]=\", sentences[0:2])\n",
    "\n",
    "for i in range (0,20):\n",
    "    print(\"X[\",i,\"]=\", X[i],\"y[\",i,\"]=\", y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142142, 14, 37)\n"
     ]
    }
   ],
   "source": [
    "#The first row is the X's first row up to 14 character\n",
    "#The second row is the X's first row starting from second character up to 14 character\n",
    "#The third row is the X's first row starting from third character up to 14 character and so on \n",
    "X=np.array(X)\n",
    "X1=np.reshape(X,(X.shape[0],X.shape[1],1))\n",
    "X1=keras.utils.to_categorical(np.array(X1), num_classes=len(char_indices))\n",
    "print(X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142142, 37)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Target Variable\n",
    "y[:10]\n",
    "#Reshapig our label for model\n",
    "y1 = np.array(y)\n",
    "# one hot encode outputs\n",
    "y1 = keras.utils.to_categorical(np.array(y), num_classes=len(char_indices))\n",
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (113713, 14, 37)\n",
      "y_train.shape (113713, 37)\n",
      "X_test.shape (28429, 14, 37)\n",
      "y_test.shape (28429, 37)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.20)\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"y_train.shape\", y_train.shape)\n",
    "print(\"X_test.shape\", X_test.shape)\n",
    "print(\"y_test.shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_4 (SimpleRNN)     (None, 16)                864       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 37)                629       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 37)                0         \n",
      "=================================================================\n",
      "Total params: 1,493\n",
      "Trainable params: 1,493\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#building the model\n",
    "model_RNN2 = Sequential()\n",
    "##model.add(SimpleRNN('number of hidden nodes in each rnn cell', input_shape=(timesteps, data_dim)))\n",
    "model_RNN2.add(SimpleRNN(16, input_shape=(X_train.shape[1], X_train.shape[2]))) \n",
    "model_RNN2.add(Dense(len(char_indices)))\n",
    "model_RNN2.add(Activation('softmax'))\n",
    "model_RNN2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 113713 samples, validate on 28429 samples\n",
      "Epoch 1/30\n",
      "113713/113713 [==============================] - 15s 132us/sample - loss: 2.2222 - accuracy: 0.3708 - val_loss: 1.9541 - val_accuracy: 0.4312\n",
      "Epoch 2/30\n",
      "113713/113713 [==============================] - 14s 126us/sample - loss: 1.9027 - accuracy: 0.4393 - val_loss: 1.8562 - val_accuracy: 0.4473\n",
      "Epoch 3/30\n",
      "113713/113713 [==============================] - 14s 126us/sample - loss: 1.8287 - accuracy: 0.4569 - val_loss: 1.8053 - val_accuracy: 0.4614\n",
      "Epoch 4/30\n",
      "113713/113713 [==============================] - 13s 113us/sample - loss: 1.7897 - accuracy: 0.4670 - val_loss: 1.7815 - val_accuracy: 0.4679\n",
      "Epoch 5/30\n",
      "113713/113713 [==============================] - 13s 111us/sample - loss: 1.7659 - accuracy: 0.4760 - val_loss: 1.7617 - val_accuracy: 0.4778\n",
      "Epoch 6/30\n",
      "113713/113713 [==============================] - 13s 112us/sample - loss: 1.7487 - accuracy: 0.4808 - val_loss: 1.7478 - val_accuracy: 0.4876\n",
      "Epoch 7/30\n",
      "113713/113713 [==============================] - 13s 111us/sample - loss: 1.7346 - accuracy: 0.4845 - val_loss: 1.7361 - val_accuracy: 0.4870\n",
      "Epoch 8/30\n",
      "113713/113713 [==============================] - 13s 111us/sample - loss: 1.7231 - accuracy: 0.4875 - val_loss: 1.7272 - val_accuracy: 0.4855\n",
      "Epoch 9/30\n",
      "113713/113713 [==============================] - 13s 112us/sample - loss: 1.7133 - accuracy: 0.4881 - val_loss: 1.7180 - val_accuracy: 0.4882\n",
      "Epoch 10/30\n",
      "113713/113713 [==============================] - 13s 111us/sample - loss: 1.7048 - accuracy: 0.4897 - val_loss: 1.7116 - val_accuracy: 0.4875\n",
      "Epoch 11/30\n",
      "113713/113713 [==============================] - 13s 112us/sample - loss: 1.6979 - accuracy: 0.4902 - val_loss: 1.7038 - val_accuracy: 0.4923\n",
      "Epoch 12/30\n",
      "113713/113713 [==============================] - 13s 112us/sample - loss: 1.6910 - accuracy: 0.4924 - val_loss: 1.6976 - val_accuracy: 0.4926\n",
      "Epoch 13/30\n",
      "113713/113713 [==============================] - 13s 111us/sample - loss: 1.6859 - accuracy: 0.4934 - val_loss: 1.6942 - val_accuracy: 0.4912\n",
      "Epoch 14/30\n",
      "113713/113713 [==============================] - 13s 111us/sample - loss: 1.6807 - accuracy: 0.4950 - val_loss: 1.6906 - val_accuracy: 0.4954\n",
      "Epoch 15/30\n",
      "113713/113713 [==============================] - 13s 113us/sample - loss: 1.6766 - accuracy: 0.4971 - val_loss: 1.6857 - val_accuracy: 0.4940\n",
      "Epoch 16/30\n",
      "113713/113713 [==============================] - 13s 113us/sample - loss: 1.6726 - accuracy: 0.4975 - val_loss: 1.6851 - val_accuracy: 0.4932\n",
      "Epoch 17/30\n",
      "113713/113713 [==============================] - 13s 112us/sample - loss: 1.6688 - accuracy: 0.4994 - val_loss: 1.6809 - val_accuracy: 0.4968\n",
      "Epoch 18/30\n",
      "113713/113713 [==============================] - 13s 112us/sample - loss: 1.6662 - accuracy: 0.4997 - val_loss: 1.6763 - val_accuracy: 0.5016\n",
      "Epoch 19/30\n",
      "113713/113713 [==============================] - 13s 111us/sample - loss: 1.6634 - accuracy: 0.5015 - val_loss: 1.6737 - val_accuracy: 0.5004\n",
      "Epoch 20/30\n",
      "113713/113713 [==============================] - 13s 112us/sample - loss: 1.6605 - accuracy: 0.5020 - val_loss: 1.6717 - val_accuracy: 0.5032\n",
      "Epoch 21/30\n",
      "113713/113713 [==============================] - 13s 113us/sample - loss: 1.6582 - accuracy: 0.5034 - val_loss: 1.6715 - val_accuracy: 0.5018\n",
      "Epoch 22/30\n",
      "113713/113713 [==============================] - 13s 111us/sample - loss: 1.6563 - accuracy: 0.5036 - val_loss: 1.6700 - val_accuracy: 0.5032\n",
      "Epoch 23/30\n",
      "113713/113713 [==============================] - 13s 112us/sample - loss: 1.6542 - accuracy: 0.5044 - val_loss: 1.6665 - val_accuracy: 0.5039\n",
      "Epoch 24/30\n",
      "113713/113713 [==============================] - 13s 112us/sample - loss: 1.6523 - accuracy: 0.5048 - val_loss: 1.6629 - val_accuracy: 0.5045\n",
      "Epoch 25/30\n",
      "113713/113713 [==============================] - 13s 113us/sample - loss: 1.6504 - accuracy: 0.5054 - val_loss: 1.6629 - val_accuracy: 0.5057\n",
      "Epoch 26/30\n",
      "113713/113713 [==============================] - 13s 113us/sample - loss: 1.6484 - accuracy: 0.5060 - val_loss: 1.6623 - val_accuracy: 0.5041\n",
      "Epoch 27/30\n",
      "113713/113713 [==============================] - 13s 112us/sample - loss: 1.6468 - accuracy: 0.5068 - val_loss: 1.6595 - val_accuracy: 0.5078\n",
      "Epoch 28/30\n",
      "113713/113713 [==============================] - 13s 111us/sample - loss: 1.6452 - accuracy: 0.5074 - val_loss: 1.6616 - val_accuracy: 0.5047\n",
      "Epoch 29/30\n",
      "113713/113713 [==============================] - 13s 112us/sample - loss: 1.6436 - accuracy: 0.5077 - val_loss: 1.6588 - val_accuracy: 0.5044\n",
      "Epoch 30/30\n",
      "113713/113713 [==============================] - 13s 113us/sample - loss: 1.6424 - accuracy: 0.5085 - val_loss: 1.6549 - val_accuracy: 0.5092\n"
     ]
    }
   ],
   "source": [
    "# compile network\n",
    "model_RNN2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model_RNN2.fit(X_train, y_train, epochs=30, verbose=1, validation_data=(X_test, y_test))\n",
    "model_RNN2.save_weights(\"char_rnn_model_weights_v1.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 113713 samples\n",
      "Epoch 1/2\n",
      "113713/113713 [==============================] - 13s 118us/sample - loss: 1.6408 - accuracy: 0.5097\n",
      "Epoch 2/2\n",
      "113713/113713 [==============================] - 13s 115us/sample - loss: 1.6395 - accuracy: 0.5094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa47028f6d0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weightsfile_model_RNN2= \"char_rnn_model_weights_v1.hdf5\"\n",
    "model_RNN2.load_weights(weightsfile_model_RNN2)\n",
    "\n",
    "# compile network\n",
    "model_RNN2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model_RNN2.fit(X_train, y_train, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to prepare test input\n",
    "def prepare_input(in_text):\n",
    "    X1 = np.array([char_indices[i] for i in in_text]).reshape(1,14,1)\n",
    "    X1=keras.utils.to_categorical(np.array(X1), num_classes=len(char_indices))\n",
    "    return(X1)\n",
    "#function to loop our preditions\n",
    "def complete_pred(in_text):\n",
    "    #original_text = in_text\n",
    "    #generated = in_text\n",
    "    completion = ''\n",
    "    while True:\n",
    "        x = prepare_input(in_text)\n",
    "        pred = model_RNN2.predict_classes(x, verbose=0)[0]\n",
    "\n",
    "        next_char = indices_char[pred]\n",
    "\n",
    "        in_text = in_text[1:] + next_char\n",
    "        completion += next_char\n",
    "\n",
    "        if len(completion)> 20 or next_char == ' ':\n",
    "            return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text --> officials say  \n",
      "predicted word --->  of \n",
      "Input text --> how dangerous  \n",
      "predicted output --->  to \n",
      "Input text --> political and  \n",
      "predicted output --->  the \n",
      "Input text --> whatever they  \n",
      "predicted output --->  of \n",
      "Input text --> of particular  \n",
      "predicted output --->  to \n"
     ]
    }
   ],
   "source": [
    "in_text = 'officials say '\n",
    "out_word = complete_pred(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\npredicted word ---> \", out_word)\n",
    "in_text = 'how dangerous '\n",
    "out_word = complete_pred(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\npredicted output ---> \", out_word)\n",
    "in_text = 'political and '\n",
    "out_word = complete_pred(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\npredicted output ---> \", out_word)\n",
    "in_text = 'whatever they '\n",
    "out_word = complete_pred(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\npredicted output ---> \", out_word)\n",
    "in_text = 'of particular '\n",
    "out_word = complete_pred(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\npredicted output ---> \", out_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128)               84992     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 37)                4773      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 37)                0         \n",
      "=================================================================\n",
      "Total params: 89,765\n",
      "Trainable params: 89,765\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#building the model\n",
    "model_LSTM = Sequential()\n",
    "#model1.add(LSTM('number of hidden nodes in each rnn cell', input_shape=(timesteps, data_dim)))\n",
    "model_LSTM.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]))) \n",
    "model_LSTM.add(Dense(len(char_indices)))\n",
    "model_LSTM.add(Activation('softmax'))\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 113713 samples\n",
      "Epoch 1/30\n",
      "113713/113713 [==============================] - 36s 319us/sample - loss: 1.9728 - accuracy: 0.4258\n",
      "Epoch 2/30\n",
      "113713/113713 [==============================] - 34s 301us/sample - loss: 1.5768 - accuracy: 0.5282\n",
      "Epoch 3/30\n",
      "113713/113713 [==============================] - 34s 302us/sample - loss: 1.4047 - accuracy: 0.5811\n",
      "Epoch 4/30\n",
      "113713/113713 [==============================] - 34s 301us/sample - loss: 1.2852 - accuracy: 0.6165\n",
      "Epoch 5/30\n",
      "113713/113713 [==============================] - 36s 316us/sample - loss: 1.1959 - accuracy: 0.6431\n",
      "Epoch 6/30\n",
      "113713/113713 [==============================] - 37s 327us/sample - loss: 1.1253 - accuracy: 0.6646\n",
      "Epoch 7/30\n",
      "113713/113713 [==============================] - 34s 303us/sample - loss: 1.0685 - accuracy: 0.6810\n",
      "Epoch 8/30\n",
      "113713/113713 [==============================] - 33s 293us/sample - loss: 1.0198 - accuracy: 0.6954\n",
      "Epoch 9/30\n",
      "113713/113713 [==============================] - 37s 330us/sample - loss: 0.9771 - accuracy: 0.7065\n",
      "Epoch 10/30\n",
      "113713/113713 [==============================] - 32s 285us/sample - loss: 0.9407 - accuracy: 0.7163\n",
      "Epoch 11/30\n",
      "113713/113713 [==============================] - 32s 283us/sample - loss: 0.9071 - accuracy: 0.7263\n",
      "Epoch 12/30\n",
      "113713/113713 [==============================] - 34s 302us/sample - loss: 0.8783 - accuracy: 0.7338\n",
      "Epoch 13/30\n",
      "113713/113713 [==============================] - 34s 303us/sample - loss: 0.8507 - accuracy: 0.7413\n",
      "Epoch 14/30\n",
      "113713/113713 [==============================] - 34s 295us/sample - loss: 0.8258 - accuracy: 0.7482\n",
      "Epoch 15/30\n",
      "113713/113713 [==============================] - 34s 298us/sample - loss: 0.8042 - accuracy: 0.7534\n",
      "Epoch 16/30\n",
      "113713/113713 [==============================] - 38s 333us/sample - loss: 0.7829 - accuracy: 0.7594\n",
      "Epoch 17/30\n",
      "113713/113713 [==============================] - 33s 286us/sample - loss: 0.7643 - accuracy: 0.7639\n",
      "Epoch 18/30\n",
      "113713/113713 [==============================] - 36s 313us/sample - loss: 0.7483 - accuracy: 0.7687\n",
      "Epoch 19/30\n",
      "113713/113713 [==============================] - 33s 287us/sample - loss: 0.7312 - accuracy: 0.7740\n",
      "Epoch 20/30\n",
      "113713/113713 [==============================] - 34s 302us/sample - loss: 0.7177 - accuracy: 0.7777\n",
      "Epoch 21/30\n",
      "113713/113713 [==============================] - 35s 306us/sample - loss: 0.7054 - accuracy: 0.7812\n",
      "Epoch 22/30\n",
      "113713/113713 [==============================] - 33s 294us/sample - loss: 0.6927 - accuracy: 0.7839\n",
      "Epoch 23/30\n",
      "113713/113713 [==============================] - 33s 287us/sample - loss: 0.6819 - accuracy: 0.7864\n",
      "Epoch 24/30\n",
      "113713/113713 [==============================] - 32s 278us/sample - loss: 0.6711 - accuracy: 0.7905\n",
      "Epoch 25/30\n",
      "113713/113713 [==============================] - 32s 277us/sample - loss: 0.6609 - accuracy: 0.7930\n",
      "Epoch 26/30\n",
      "113713/113713 [==============================] - 31s 277us/sample - loss: 0.6518 - accuracy: 0.7948\n",
      "Epoch 27/30\n",
      "113713/113713 [==============================] - 32s 277us/sample - loss: 0.6426 - accuracy: 0.7966\n",
      "Epoch 28/30\n",
      "113713/113713 [==============================] - 32s 278us/sample - loss: 0.6349 - accuracy: 0.7997\n",
      "Epoch 29/30\n",
      "113713/113713 [==============================] - 32s 277us/sample - loss: 0.6269 - accuracy: 0.8012\n",
      "Epoch 30/30\n",
      "113713/113713 [==============================] - 32s 278us/sample - loss: 0.6201 - accuracy: 0.8034\n"
     ]
    }
   ],
   "source": [
    "# compile network\n",
    "model_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model_LSTM.fit(X_train, y_train, epochs=30, verbose=1)\n",
    "model_LSTM.save_weights(\"char_LSTM_model_weights_v1.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'Datasets/Pre_trained_models/char_LSTM_model_weights_v1.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-fd58c1b2ec57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mweightsfile_model_LSTM\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mData_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/Pre_trained_models/char_LSTM_model_weights_v1.hdf5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_LSTM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mweightsfile_model_LSTM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# compile network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_LSTM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m    179\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    180\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   1169\u001b[0m           'first, then load the weights.')\n\u001b[1;32m   1170\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1172\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'Datasets/Pre_trained_models/char_LSTM_model_weights_v1.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "weightsfile_model_LSTM= \"char_LSTM_model_weights_v1.hdf5\"\n",
    "model_LSTM.load_weights( weightsfile_model_LSTM)\n",
    "\n",
    "# compile network\n",
    "model_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model_LSTM.fit(X_train, y_train,epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to prepare test input\n",
    "def prepare_input1(in_text):\n",
    "    X1 = np.array([char_indices[i] for i in in_text]).reshape(1,14,1)\n",
    "    X1= keras.utils.to_categorical(np.array(X1), num_classes=len(char_indices))\n",
    "    return(X1)\n",
    "#function to loop our preditions\n",
    "def complete_pred1(in_text):\n",
    "    #original_text = in_text\n",
    "    #generated = in_text\n",
    "    completion = ''\n",
    "    while True:\n",
    "        x = prepare_input1(in_text)\n",
    "        pred = model_LSTM.predict_classes(x, verbose=0)[0]\n",
    "        next_char = indices_char[pred]\n",
    "\n",
    "        in_text = in_text[1:] + next_char\n",
    "        completion += next_char\n",
    "\n",
    "        if len(completion)> 20 or next_char == ' ':\n",
    "            return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_text = 'the emergence '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n",
    "in_text = 'officials say '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n",
    "in_text = 'and sentenced '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n",
    "in_text = 'a combination '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)\n",
    "in_text = 'and according '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"; predicted output ---> \", out_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few more predictions and Comparions with Standard RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_text = 'how dangerous '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\nLSTM Prediction ---> \", out_word)\n",
    "out_word1 = complete_pred(in_text)\n",
    "print(\"RNN Prediction ---> \", out_word1)\n",
    "\n",
    "print(\"\\n\")\n",
    "in_text = 'political and '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\nLSTM Prediction ---> \", out_word)\n",
    "out_word1 = complete_pred(in_text)\n",
    "print(\"RNN Prediction ---> \", out_word1)\n",
    "\n",
    "print(\"\\n\")\n",
    "in_text = 'of particular '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\nLSTM Prediction ---> \", out_word)\n",
    "out_word1 = complete_pred(in_text)\n",
    "print(\"RNN Prediction ---> \", out_word1)\n",
    "\n",
    "print(\"\\n\")\n",
    "in_text = 'whatever they '\n",
    "out_word = complete_pred1(in_text)\n",
    "print(\"Input text -->\", in_text, \"\\nLSTM Prediction ---> \", out_word)\n",
    "out_word1 = complete_pred(in_text)\n",
    "print(\"RNN Prediction ---> \", out_word1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study â€“ Language Translation Project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data= open(Data_path+\"/fra-eng/fra.txt\", mode='rt', encoding='utf-8').read()\n",
    "raw_data=raw_data.strip().split('\\n')\n",
    "raw_data=[i.split('\\t') for i in raw_data]\n",
    "lang1_lang2_data=array(raw_data)\n",
    "print(lang1_lang2_data)\n",
    "print(\"Overall pairs\", len(lang1_lang2_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "lang1_lang2_data[:,0] = [word.translate(str.maketrans('', '', string.punctuation)) for word in lang1_lang2_data[:,0]]\n",
    "lang1_lang2_data[:,1] = [word.translate(str.maketrans('', '', string.punctuation)) for word in lang1_lang2_data[:,1]]\n",
    "\n",
    "print(lang1_lang2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert text to lowercase\n",
    "for word in range(len(lang1_lang2_data)):\n",
    "    lang1_lang2_data[word,0] = lang1_lang2_data[word,0].lower()\n",
    "    lang1_lang2_data[word,1] = lang1_lang2_data[word,1].lower()\n",
    "print(lang1_lang2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lang1_lang2_data[:, 0])\n",
    "lang1_tokens=tokenizer\n",
    "lang1_vocab_size = len(lang1_tokens.word_index) + 1\n",
    "print(\"lang1_vocab_size\", lang1_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lang1_lang2_data[:, 1])\n",
    "lang2_tokens=tokenizer\n",
    "lang2_vocab_size = len(lang2_tokens.word_index) + 1\n",
    "print(\"lang2_vocab_size\", lang2_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test set\n",
    "train, test = train_test_split(lang1_lang2_data, test_size=0.1, random_state = 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang1_seq_length=15\n",
    "lang2_seq_length=15\n",
    "\n",
    "X_train_seq=lang1_tokens.texts_to_sequences(train[:, 0])\n",
    "X_train= pad_sequences(X_train_seq,lang1_seq_length,padding='post')\n",
    "\n",
    "Y_train_seq=lang2_tokens.texts_to_sequences(train[:, 1])\n",
    "Y_train= pad_sequences(Y_train_seq,lang2_seq_length,padding='post')\n",
    "\n",
    "X_test_seq=lang1_tokens.texts_to_sequences(test[:, 0])\n",
    "X_test= pad_sequences(X_test_seq,lang1_seq_length,padding='post')\n",
    "\n",
    "Y_test_seq=lang2_tokens.texts_to_sequences(test[:, 1])\n",
    "Y_test= pad_sequences(Y_test_seq,lang2_seq_length,padding='post')\n",
    "\n",
    "print(\"X_train.shape\", X_train.shape)\n",
    "print(\"Y_train.shape\",Y_train.shape)\n",
    "print(\"X_test.shape\",X_test.shape)\n",
    "print(\"Y_test.shape\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Text data\", [train[5, 0]])\n",
    "print('Numbers sequence', X_train_seq[5])\n",
    "print('Padded Sequence', X_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(lang1_vocab_size, 256, input_length=lang1_seq_length, mask_zero=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(RepeatVector(lang2_seq_length))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dense(lang2_vocab_size, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "history = model.fit(X_train, Y_train.reshape(Y_train.shape[0], Y_train.shape[1], 1),  epochs=1, verbose=1, batch_size=1024)\n",
    "model.save_weights('Eng_fra_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(Data_path+\"/Pre_trained_models/Eng_fra_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_line_prediction(text1):\n",
    "    \n",
    "    def to_lines(text):\n",
    "          sents = text.strip().split('\\n')\n",
    "          sents = [i.split('\\t') for i in sents]\n",
    "          return sents\n",
    "    small_input = to_lines(text1)\n",
    "    small_input = array(small_input)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    small_input[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in small_input[:,0]]\n",
    "    # convert text to lowercase\n",
    "    for i in range(len(small_input)):\n",
    "        small_input[i,0] = small_input[i,0].lower()\n",
    "\n",
    "    #encode and pad sequences\n",
    "    small_input_seq=lang1_tokens.texts_to_sequences(small_input[0])\n",
    "    small_input= pad_sequences(small_input_seq,lang1_seq_length,padding='post')\n",
    "   \n",
    "\n",
    "    #Load the model\n",
    "    #Eng French Model\n",
    "    #model.load_weights('/content/drive/My Drive/Training/Book/0.Chapters/Chapter12 RNN and LSTM/1.Archives/Eng_fra_model_v2.hdf5')\n",
    "\n",
    "    pred_seq = model.predict_classes(small_input[0:1].reshape((small_input[0:1].shape[0],small_input[0:1].shape[1])))\n",
    "    \n",
    "    def num_to_word(n, tokens):\n",
    "          for word, index in tokens.word_index.items():\n",
    "              if index == n:\n",
    "                  return word\n",
    "          return None\n",
    "\n",
    "    Lang2_text = []\n",
    "    for word_num in pred_seq:\n",
    "          sing_pred = []\n",
    "          for i in range(len(word_num)):\n",
    "                t = num_to_word(word_num[i], lang2_tokens)\n",
    "                if i > 0:\n",
    "                    if (t == num_to_word(word_num[i-1], lang2_tokens)) or (t == None):\n",
    "                        sing_pred.append('')\n",
    "                    else:\n",
    "                        sing_pred.append(t)\n",
    "                else:\n",
    "                      if(t == None):\n",
    "                              sing_pred.append('')\n",
    "                      else:\n",
    "                              sing_pred.append(t) \n",
    "          Lang2_text.append(' '.join(sing_pred))\n",
    "    return(Lang2_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_sentences=[\"have a great Good day\",\n",
    "                 \"Do you speak English\",\n",
    "                 \"I do not know your language\",\n",
    "                 \"I need help\",\n",
    "                 \"Thank you very much\",\n",
    "                 \"Where can I get this\",\n",
    "                 \"How much does it cost\",\n",
    "                 \"Where is the bathroom\",\n",
    "                 \"Where is the ATM\",\n",
    "                 \"I am a visitor here\",\n",
    "                 \"Excuse me\",\n",
    "                 \"What do you do for living\",\n",
    "                 \"Here is my passport\"]\n",
    "\n",
    "for sent in Input_sentences:\n",
    "  print([sent] , \" -->\",one_line_prediction(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
